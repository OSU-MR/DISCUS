{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "578a5000",
   "metadata": {},
   "source": [
    "# DISCUS Implementation for LGE data: Studies (III, IV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12034a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################## Import libraries\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.io import savemat\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "import numpy as np\n",
    "import pickle\n",
    "#from models.resnet import ResNet\n",
    "# from models.unet import UNet\n",
    "from models.skip_dropout_LatestOrder import skip\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "from utils.common_utils import *\n",
    "from utils.my_utils import *\n",
    "from utils.fftc import * # ra: added the pytorch fft routine from fastmri\n",
    "\n",
    "import os# os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark =True\n",
    "dtype = torch.cuda.FloatTensor\n",
    "PLOT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2f00eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check available hardware:\n",
    "def list_cuda_devices():\n",
    "    if torch.cuda.is_available():\n",
    "        num_devices = torch.cuda.device_count()\n",
    "        print(f\"Number of available CUDA devices: {num_devices}\")\n",
    "        for i in range(num_devices):\n",
    "            print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "            print(f\"  Memory Allocated: {torch.cuda.memory_allocated(i) / 1024 ** 2:.2f} MB\")\n",
    "            print(f\"  Memory Cached: {torch.cuda.memory_reserved(i) / 1024 ** 2:.2f} MB\")\n",
    "            print(f\"  Total Memory: {torch.cuda.get_device_properties(i).total_memory / 1024 ** 2:.2f} MB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available.\")\n",
    "\n",
    "list_cuda_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1d2215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.cuda.device_count()) # restart shell if it doesnt show 2\n",
    "\n",
    "# torch.cuda.set_device(0) # 0/1\n",
    "\n",
    "\n",
    "# print(torch.cuda.current_device())\n",
    "# # Change directory\n",
    "# %pwd # check current directory\n",
    "# # Move the directory where the data is\n",
    "# import os\n",
    "# os.chdir(\"/home/ahmad.sultan/.cache/gvfs/smb-share:server=ak-isi01-sh2.prdnas1.osumc.edu,share=dhlri$/labs/CMRCT Lab Team/_ahmad_sultan/_shared_GPU_station/LGE Patient Datasets Preprocessing\")\n",
    "# %ls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d671ed",
   "metadata": {},
   "source": [
    "## Set study parameters and select dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2910127e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters:\n",
    "FS = 0 # retrospective or prospective \n",
    "R=2 # if FS=0, \n",
    "nt= 32 # choose num of frames to recon. Max = 32.\n",
    "\n",
    "# a list of all patient file names: [\"SJK_SAX_BASE\", \"SJK_SAX_MOCO\", \"SJK_3CH\", \"JBH_3CH\", \"JGR_3CH\", \"GRH_3CH\", \"AT_3CH\", \"JBH_3CH\"]\n",
    "id=\"JBH_3CH\"  # choose one of the 8 preprocessed patients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18c42f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## based on ID above, data file names are selected automatically. You can specify yours file too.\n",
    "if not FS:\n",
    "    R=5.069307\n",
    "# # arbitrary id\n",
    "# id = \"JB_3CH\" #'SJK_SAX_MOCO' #SJK_SAX_BASE # SAX # 2CH # 3CH\n",
    "crop_set = 0.9\n",
    "if id[0:3]==\"SJK\":\n",
    "   subject = \"20230125_CS_LGE_SKJ/\" #\"20230125_CS_LGE_SKJ/\"\n",
    "   if not FS:\n",
    "      if id==\"SJK_SAX_BASE\":\n",
    "         file = \"/meas_MID00257_FID117434_SS_TRUFI_CS_PSIR_SAX_BASE_SAX/\" #meas_MID00381_FID115208_SS_TRUFI_CS_PSIR_3CH\n",
    "      elif id==\"SJK_SAX_MOCO\":\n",
    "         file = \"/meas_MID00258_FID117435_SS_TRUFI_CS_PSIR_SAX_SAX_MOCO_54_6/\" #meas_MID00381_FID115208_SS_TRUFI_CS_PSIR_3CH\n",
    "      elif id==\"SJK_3CH\":\n",
    "         file = \"/meas_MID00259_FID117436_SS_TRUFI_CS_PSIR_SAX_3CH/\" #meas_MID00381_FID115208_SS_TRUFI_CS_PSIR_3CH\n",
    "\n",
    "   else:\n",
    "      if id==\"SJK_SAX_BASE\":\n",
    "         file = \"/meas_MID00260_FID117437_SS_TRUFI_PSIR_SAX_FULL_SAMP_BASE_SAX/\" #meas_MID00381_FID115208_SS_TRUFI_CS_PSIR_3CH\n",
    "      elif id==\"SJK_SAX_MOCO\":\n",
    "         file = \"/meas_MID00261_FID117438_SS_TRUFI_PSIR_SAX_FULL_SAMP_SAX_MOCO_54_6/\" #meas_MID00381_FID115208_SS_TRUFI_CS_PSIR_3CH\n",
    "      elif id==\"SJK_3CH\":\n",
    "         file = \"/meas_MID00262_FID117439_SS_TRUFI_PSIR_SAX_FULL_SAMP_3CH/\" #meas_MID00381_FID115208_SS_TRUFI_CS_PSIR_3CH\n",
    "\n",
    "elif id[0:3]==\"JBH\":\n",
    "   subject = \"20230118_CS_LGE_JB/\" #\"20230125_CS_LGE_SKJ/\"\n",
    "   if not FS:\n",
    "      if id==\"JBH_3CH\":\n",
    "         crop_set=0.8\n",
    "         file = \"/meas_MID00381_FID115208_SS_TRUFI_CS_PSIR_3CH/\" #meas_MID00381_FID115208_SS_TRUFI_CS_PSIR_3CH\n",
    "   else:\n",
    "      if id==\"JBH_3CH\":\n",
    "         file = \"/meas_MID00383_FID115210_SS_TRUFI_PSIR_3CH_FULL_SAMP/\" #meas_MID00381_FID115208_SS_TRUFI_CS_PSIR_3CH\n",
    "\n",
    "elif id[0:3]==\"JGR\":\n",
    "   subject = \"20230308_CS_LGE_JGR/\" #\"20230125_CS_LGE_SKJ/\"\n",
    "   if not FS:\n",
    "      crop_set = 0.6\n",
    "      R=6.336634\n",
    "      if id==\"JGR_3CH\":\n",
    "         file = \"/meas_MID00097_FID134147_SS_TRUFI_CS_PSIR_3CH/\" #meas_MID00381_FID115208_SS_TRUFI_CS_PSIR_3CH\n",
    "      elif id==\"JGR_SAX_BASE\":\n",
    "         file = \"/meas_MID00096_FID134146_SS_TRUFI_CS_PSIR_BASE_SAX/\" #meas_MID00381_FID115208_SS_TRUFI_CS_PSIR_3CH\n",
    "\n",
    "elif id[0:3]==\"GRH\":\n",
    "   subject = \"20230118_CS_LGE_GR/\" #\"20230125_CS_LGE_SKJ/\"\n",
    "   if not FS:\n",
    "      crop_set=0.8\n",
    "      R=7.603960\n",
    "      if id==\"GRH_3CH\":\n",
    "         file = \"/meas_MID00488_FID115315_SS_TRUFI_CS_PSIR_3CH/\" #meas_MID00381_FID115208_SS_TRUFI_CS_PSIR_3CH\n",
    "\n",
    "elif id[0:2]==\"AT\":\n",
    "   subject = \"20230816_CS_LGE_AT/\" #\"20230125_CS_LGE_SKJ/\"\n",
    "   if not FS:\n",
    "      if id==\"AT_3CH\":\n",
    "         file = \"/meas_MID00400_FID199583_SS_TRUFI_CS_PSIR_3_CH_SLICE_54_5/\" #meas_MID00381_FID115208_SS_TRUFI_CS_PSIR_3CH\n",
    "\n",
    "folder = \"\"#\"new sen cc maps/\" # for new sen cc maps\n",
    "data_path = \"./data/preprocessed/\"+subject +folder+file # for new sen cc maps: folder\n",
    "# data_path0 = data_path+\"set1/\"\n",
    "# set=0 # set=0 for T1 (with contrast)\n",
    "if FS:\n",
    "    data_path_r = data_path + \"R%d/\"%R\n",
    "else:\n",
    "    data_path_r = data_path\n",
    "# else: # US case\n",
    "# crop_set= 0.9 #[0.2,0.3]\n",
    "set=0\n",
    "crop=crop_set #[set]\n",
    "## display\n",
    "gm = [1, 0.3, 0.7] # gamma correction for display\n",
    "g=1\n",
    "mps = [plt.cm.Greys_r, 'gray']\n",
    "epsilon = 1e-10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb41671",
   "metadata": {},
   "source": [
    "## Load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089a6a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read US data:\n",
    "# print(data_path_r+\"yu\" +\"_R_%f\"%R+\"_set_%d\"%set+\".npy\")\n",
    "ksp = np.load(data_path_r+\"yu\" +\"_R_%f\"%R+\"_set_%d\"%set+\".npy\")\n",
    "print(\"Data size: \", ksp.shape)\n",
    "(N,Nc,Nx,Ny) = np.shape(ksp)\n",
    "n = (Nx, Ny)\n",
    "RO_offset = int(0.2*Nx) # skip 25 RO rows from each top and bottom of image when calculating metrics to discard inhomogenity\n",
    "# R=2\n",
    "# print(data_path_r+\"mask_R_%f\"%R+\".npy\")\n",
    "msk = np.load(data_path_r+\"mask_R_%f\"%R+\".npy\")\n",
    "# print(msk.shape, msk.dtype)\n",
    "# print(\"MAPS path: \" +data_path+\"sen_cc_map\"+\".npy\")\n",
    "cc_maps_sens = np.load(data_path+\"sen_cc_map\"+\".npy\")\n",
    "# print(cc_maps_sens.shape, cc_maps_sens.dtype)\n",
    "# print(\"MAPS path: \" +data_path_r+\"ESPIRiT_crop_%f\"%crop+ '_R_%f'%R+\"_set_%d\"%set +\".npy\")\n",
    "maps = np.load(data_path_r+\"ESPIRiT_crop_%f\"%crop+ '_R_%f'%R+\"_set_%d\"%set +\".npy\")\n",
    "# print(maps.shape, maps.dtype)\n",
    "sen_msk = np.load(data_path_r+\"th_sen_map_crop_%f\"%crop+'_R_%f'%R+\"_set_%d\"%set+\".npy\")\n",
    "# print(sen_msk.shape, sen_msk.dtype)\n",
    "if FS:\n",
    "    img = np.load(data_path+\"xRef_N_%d\"%N+\"_set_%d\"%set+\".npy\")\n",
    "    # print(img.shape, img.dtype)\n",
    "    # print(np.max(np.abs(img)), np.min(np.abs(img)))\n",
    "ku = complex_to_real_plus_imag_4d(ksp)\n",
    "samp = msk\n",
    "sen = complex_to_real_plus_imag_4d(np.expand_dims(maps,axis=0)) * cc_maps_sens\n",
    "print(\"\\ndata sizes going to DISCUS\")\n",
    "print(ku.shape) # (2*N, CH, RO, PE)\n",
    "print(samp.shape) # (N, CH, RO, PE)\n",
    "print(sen.shape) # (2, CH, RO, PE)\n",
    "if FS:\n",
    "    m = complex_to_real_plus_imag_3d(img)\n",
    "    # print(m.shape) # (2*N, RO, PE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8e5a7b",
   "metadata": {},
   "source": [
    "## CS  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4deb51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_CS = 1\n",
    "show_every_CS = 100\n",
    "\n",
    "\n",
    "mu = 1e-1 # lagrangian parameter\n",
    "nIter = np.array([100, 5]) # outer and inner iterations\n",
    "ss = 0.3 # step size\n",
    "\n",
    "# tau=0.005\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bba18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################## CS Recon\n",
    "# get the start time\n",
    "# st = time.time()\n",
    "\n",
    "\n",
    "csRe=1\n",
    "\n",
    "ar = 0.75 # aspect ratio for plotting xAbs\n",
    "\n",
    "if csRe==1:\n",
    "  # Running ADMM L2-L1\n",
    "\n",
    "  xCS = np.zeros((2*N,n[0],n[1]))\n",
    "  Sc = ScN\n",
    "  # xCSAbs = np.zeros((N,n[0],n[1]))\n",
    "  # if FS:\n",
    "  #   nmse = np.zeros([N,1])\n",
    "  #   ssm = np.zeros([N,1])\n",
    "  #   psn = np.zeros([N,1])\n",
    "  #   xAbs = np.zeros((N,n[0],n[1]))\n",
    "  #   errMap = np.zeros((2*N,n[0],n[1]))\n",
    "\n",
    "  # xCSF = np.zeros((2*N,n[0],n[1]))\n",
    "  # xCSFAbs = np.zeros((N,n[0],n[1]))\n",
    "  # # yAbs = np.zeros((N,n[0],n[1]))\n",
    "  # # errFMap = np.zeros((2*N,n[0],n[1]))\n",
    "\n",
    "\n",
    "# CS-L1: individual recon (frame by frame); no common support bw frames\n",
    "  for i in range(N): \n",
    "    loss = np.zeros(nIter[0]) # track training loss to see stability of learning\n",
    "\n",
    "    print('\\nSlice: %2d' %(i+1), 'out of %2d' %N)\n",
    "    x0 = (np.zeros(n)).astype(complex)\n",
    "#     print(yuN.shape)\n",
    "#     print(mskN.shape)\n",
    "    y0 = yuN[2*i,:,:,:] + 1j*yuN[2*i+1,:,:,:]\n",
    "    msk0 = mskN[i,:,:,:]\n",
    "    # Sc = ScN[i,:,:,:]\n",
    "    \n",
    "\n",
    "    # ADMM\n",
    "    #xTmp = admm_pmri_l1(x0, y0, msk0, ScN, nIter, ss, mu, tau) ## returns reconstructed x only (no weights i.e., d, b, )\n",
    "    # x = x0\n",
    "    # y = y0\n",
    "\n",
    "    B = 4 # wavelet bands\n",
    "    u = pAt(y0, msk0, Sc) # starts with ZF-image\n",
    "    # print(u.shape)\n",
    "    d = np.zeros((B,x0.shape[0],x0.shape[1])).astype(complex)\n",
    "    b = np.zeros((B,x0.shape[0],x0.shape[1])).astype(complex)\n",
    "\n",
    "    # ii=0 # iterations\n",
    "\n",
    "    for l in range(nIter[0]): # outer iter\n",
    "\n",
    "\n",
    "      for j in range(nIter[1]): # inner iter\n",
    "\n",
    "        lossA = pA(u, msk0, Sc) - y0 # Data consistency\n",
    "        gradA = pAt(lossA, msk0, Sc)\n",
    "\n",
    "        lossW = swt2_haar(u) - d + b # wavelet reg.\n",
    "        gradW = mu * iswt2_haar(lossW) \n",
    "\n",
    "        # loss[l] = np.sum(np.abs(lossA)**2) + mu*np.sum(np.abs(lossW))\n",
    "\n",
    "        u = u - ss * (gradA + gradW) # grad. update\n",
    "\n",
    "        # ii = ii + 1\n",
    "\n",
    "\n",
    "\n",
    "        # xAbs[i:i+1,:,:] = np.sqrt((xN[i*2,:,:])**2 + (xN[i*2+1,:,:])**2)\n",
    "        # errMap[i*2:(i+1)*2,:,:] = xN[i*2:(i+1)*2,:,:]-xCS[i*2:(i+1)*2,:,:]\n",
    "\n",
    "        # nmse[i] =  np.mean((xN[i*2:(i+1)*2,:,:]-xCS[i*2:(i+1)*2,:,:])**2) / np.mean((xN[i*2:(i+1)*2,:,:])**2)\n",
    "        # ssm[i] = ssim(xCSAbs[i,:,:], xAbs[i,:,:], data_range = xCSAbs[i,:,:].max() - xCSAbs[i,:,:].min()) # xCSL1Abs[i:i+1,:,:].max() - xCSL1Abs[i:i+1,:,:].min()\n",
    "        \n",
    "        # print('\\nNMSE : %1.2f' %(10*np.log10(nmse[i])))\n",
    "        # print('SSIM : %1.2f' %(ssm[i]))\n",
    "\n",
    "\n",
    "      # recon weights learnt: d, b\n",
    "      d = st(swt2_haar(u) + b, tau/mu) \n",
    "      b = b + (swt2_haar(u) - d)\n",
    "      \n",
    "      loss[l] = np.sum(np.abs(lossA)**2) + mu*np.sum(np.abs(lossW))\n",
    "    \n",
    "    \n",
    "      if PLOT_CS:\n",
    "\n",
    "\n",
    "        if  (l+1) % show_every_CS ==0:\n",
    "          print('Iteration: %2d ' %(l+1))\n",
    "\n",
    "          xHat_it = np.zeros((2,n[0],n[1]))\n",
    "          if FS:\n",
    "            xHat_itAbs = np.zeros((1, n[0],n[1]))\n",
    "            x_itAbs = np.zeros((1, n[0],n[1]))\n",
    "\n",
    "                # recon image: u\n",
    "          xHat_it[0,:,:]  = np.real(u)\n",
    "          xHat_it[1,:,:] = np.imag(u)\n",
    "          xHat_itAbs = np.sqrt((xHat_it[0,:,:])**2 + (xHat_it[1,:,:])**2)\n",
    "          if FS:\n",
    "            x_itAbs = np.sqrt((xN[i*2,:,:])**2 + (xN[i*2+1,:,:])**2)\n",
    "            nmse_it =  np.mean((xN[i*2:(i+1)*2,:,:]-xHat_it)**2) / np.mean((xN[i*2:(i+1)*2,:,:])**2)\n",
    "            ssm_it = ssim(xHat_itAbs, x_itAbs, data_range = xHat_itAbs.max() - xHat_itAbs.min()) # xCSL1Abs[i:i+1,:,:].max() - xCSL1Abs[i:i+1,:,:].min()\n",
    "            psn_it = psnr(x_itAbs, xHat_itAbs, data_range = xHat_itAbs.max() - xHat_itAbs.min()) # xCSL1Abs[i:i+1,:,:].max() - xCSL1Abs[i:i+1,:,:].min()\n",
    "            print('\\nNMSE : %1.2f' %(10*np.log10(nmse_it)))\n",
    "            print('SSIM : %1.2f' %(ssm_it))\n",
    "            print('PSNR : %1.2f' %(psn_it))\n",
    "            fig = plt.figure(figsize=(6,5),facecolor='white', edgecolor=None)\n",
    "            plt.imshow(np.reshape(np.transpose(np.concatenate((np.expand_dims(xHat_itAbs**g, axis=0), np.expand_dims(x_itAbs**g, axis=0)), axis=2),[1,0,2]), [n[0],n[1]*2]), cmap=plt.cm.Greys_r, aspect=(n[1]/n[0])/ar) # use a specific color map\n",
    "            plt.title(\"xHat_itAbs, x_itAbs\")\n",
    "            plt.show()\n",
    "          else:\n",
    "            fig = plt.figure(figsize=(6,5),facecolor='white', edgecolor=None)\n",
    "            plt.imshow(xHat_itAbs**g, cmap=plt.cm.Greys_r, aspect=(n[1]/n[0])/ar) # use a specific color map\n",
    "            plt.title(\"xHat_itAbs\")\n",
    "            plt.show()    \n",
    "\n",
    "        if i==N-1 and l==nIter[0]-1: # last outer iter.\n",
    "          fig = plt.figure(figsize=(16,5),facecolor='white', edgecolor=None)\n",
    "          plt.plot(np.log10(loss))\n",
    "          plt.xlabel(\"No. of iterations\")\n",
    "          plt.ylabel(\"Total loss\")\n",
    "          plt.show()\n",
    "\n",
    "\n",
    "    #return u # recon image\n",
    "    # xTmp = u\n",
    "\n",
    "  \n",
    "\n",
    "    xCS[2*i,:,:]   = np.real(u)\n",
    "    xCS[2*i+1,:,:] = np.imag(u)\n",
    "    # xCSAbs[i:i+1,:,:] = np.sqrt((xCS[i*2,:,:])**2 + (xCS[i*2+1,:,:])**2)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # fTmp = np.fft.fftshift(np.fft.fft2(np.fft.ifftshift(xTmp, axes=(-2, -1)), axes=(-2, -1), norm='ortho'), axes=(-2, -1))\n",
    "    # xCSF[2*i,:,:]   = np.real(fTmp)\n",
    "    # xCSF[2*i+1,:,:] = np.imag(fTmp)\n",
    "    # xCSFAbs[i:i+1,:,:] = np.sqrt((xCSF[i*2,:,:])**2 + (xCSF[i*2+1,:,:])**2)\n",
    "    # yAbs[i:i+1,:,:] = np.sqrt((yN[i*2,:,:])**2 + (yN[i*2+1,:,:])**2)\n",
    "    # errFMap[i*2:(i+1)*2,:,:] = yN[i*2:(i+1)*2,:,:]-xCSF[i*2:(i+1)*2,:,:]\n",
    "\n",
    "  xCSAbs = takeMag(xCS)\n",
    "\n",
    "  if FS:\n",
    "    xAbs = takeMag(xN)\n",
    "    errMap = xN-xCS\n",
    "    nmse =  calc_nmse(real_plus_imag_to_complex_3d(xCS), real_plus_imag_to_complex_3d(xN))\n",
    "    # ssm[i] = ssim(xCSAbs[i,:,:], xAbs[i,:,:], data_range = xCSAbs[i,:,:].max() - xCSAbs[i,:,:].min()) # xCSL1Abs[i:i+1,:,:].max() - xCSL1Abs[i:i+1,:,:].min()\n",
    "    # psn[i] = psnr( xAbs[i,:,:], xCSAbs[i,:,:], data_range = xCSAbs[i,:,:].max() - xCSAbs[i,:,:].min()) # xCSL1Abs[i:i+1,:,:].max() - xCSL1Abs[i:i+1,:,:].min()\n",
    "    # print('\\nNMSE : %1.2f' %(10*np.log10(nmse[i])))\n",
    "    # print('SSIM : %1.2f' %(ssm[i]))\n",
    "    # print('PSNR : %1.2f' %(psn[i]))\n",
    "\n",
    "    \n",
    "  fig = plt.figure(figsize=(16,8),facecolor='white', edgecolor=None)\n",
    "  #plt.imshow(np.reshape(np.transpose(xCSAbs**0.9,[1,0,2]), [n[0],n[1]*N]), vmin=0, vmax=0.5*np.max(xCSAbs**0.9), cmap=plt.cm.Greys_r, aspect=(n[1]/n[0])/ar) # use a specific color map\n",
    "  plt.imshow(np.reshape(np.transpose(xCSAbs**g,[1,0,2]), [n[0],n[1]*N]), cmap=plt.cm.Greys_r) # use a specific color map\n",
    "  plt.title(\"xCSAbs\")\n",
    "  plt.show()\n",
    "    \n",
    "  if FS:\n",
    "    print('\\nMean nmse: %1.2f,' %(10*np.log10(np.mean(nmse))), 'nmse: ',', '.join('%1.2f' % (10*np.log10(nmse[j])) for j in range(len(nmse)))) \n",
    "    # print('Mean ssim: %1.3f,' %(np.mean(ssm)), 'ssim: ',', '.join('%1.3f' % (ssm[j]) for j in range(len(ssm)))) \n",
    "    # print('Mean psnr: %1.3f,' %(np.mean(psn)), 'psnr: ',', '.join('%1.3f' % (psn[j]) for j in range(len(psn)))) \n",
    "   \n",
    "    fig = plt.figure(figsize=(16,8),facecolor='white', edgecolor=None)\n",
    "    plt.imshow(np.reshape(np.transpose(xAbs**g,[1,0,2]), [n[0],n[1]*N]), cmap=mps[0]) # use a specific color map\n",
    "    plt.title(\"xAbs\")\n",
    "    plt.show()\n",
    "  #   fig = plt.figure(figsize=(16,8),facecolor='white', edgecolor=None)\n",
    "  #   plt.imshow(np.reshape(np.transpose(np.concatenate((xAbs[0:1,:,:]**0.99, xCSAbs[0:1,:,:]**0.99), axis=2),[1,0,2]), [n[0],n[1]*2]), cmap=plt.cm.Greys_r) # use a specific color map\n",
    "  #   plt.title(\"xAbs, xCSAbs\")  \n",
    "  #   plt.show()\n",
    "\n",
    "      \n",
    "    fig = plt.figure(figsize=(16,8),facecolor='white', edgecolor=None)\n",
    "    plt.imshow(np.reshape(np.transpose(takeMag(errMap),[1,0,2]), [n[0],n[1]*N]),vmin=0, vmax=(np.max(errMap)/5), cmap=plt.cm.Greys_r) # use a specific color map\n",
    "    plt.title(\"errxMap\")\n",
    "    plt.show()\n",
    "\n",
    "  if FS:\n",
    "    ### NMSE on ROI:\n",
    "      print(\"NMSE on ROI: \")\n",
    "      RO_offset = int(0.2*Nx)\n",
    "      print(RO_offset)\n",
    "      nmse =  calc_nmse(real_plus_imag_to_complex_3d(xCS[:,RO_offset:-RO_offset,:]), real_plus_imag_to_complex_3d(xN[:,RO_offset:-RO_offset,:]))\n",
    "      print('\\nMean nmse: %1.2f,' %(10*np.log10(np.mean(nmse))), 'nmse: ',', '.join('%1.2f' % (10*np.log10(nmse[j])) for j in range(len(nmse)))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1599a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sv_CS=1\n",
    "CSresults_path=data_path_r\n",
    "string = \"sen_cc_\"\n",
    "if sv_CS==1: np.save(CSresults_path + string+'xHatCSL1_'+'N_%d' % N + '_R_%f'%R +\"_set_%d\"%set+\"_tau_%f\"%tau+'.npy', xCS)\n",
    "if FS:\n",
    "    if sv_CS==1: np.save(CSresults_path + string+ 'xRefCSL1_'+'N_%d' % N + '_R_%f'%R+\"_set_%d\"%set+'.npy', xN)\n",
    "    # if sv_CS==1: np.save(CSresults_path + string+ 'nmseCSL1_'+'N_%d' % N + '_R_%f'%R+\"_set_%d\"%set +'.npy', nmse)\n",
    "    # if sv_CS==1: np.save(CSresults_path + 'ssimCSL1_'+'N_%d' % N + '_R_%f'%R +'.npy', ssm)\n",
    "    # if sv_CS==1: np.save(CSresults_path + 'psnrCSL1_'+'N_%d' % N + '_R_%f'%R +'.npy', psn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7f9cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cd209e3",
   "metadata": {},
   "source": [
    "## DISCUS training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2e516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc6a33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DISCUS Archit. Parameters:\n",
    "\n",
    "opt = 2 # select the flavor of the algorithm\n",
    "LSz = 128 # Number of channels in hidden layers\n",
    "NLy = 6 # number of layers\n",
    "Nm = 2**NLy # minimum matrix size for UNet # depends on giveb data matrix size\n",
    "# code accepts both even and odd sized matrix sizes\n",
    "\n",
    "num_iter = 12000#120#12000 # number of iterations\n",
    "show_every = 1000\n",
    "\n",
    "## tuning parameters...\n",
    "# code vectors init\n",
    "reg_sig0 = 0.01 #0.03 # noise regularization \n",
    "#code vector sparsity\n",
    "z_scale = 1\n",
    "\n",
    "# adam optimization\n",
    "WtD = 1*1e-6 # weight decay \n",
    "p=0 #0.01 # keep drop_out\n",
    "\n",
    "sv=0\n",
    "cf=30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13581aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for DISCUS\n",
    "## Final pre-processed data:\n",
    "N=nt\n",
    "\n",
    "if FS:\n",
    "    xN  = m[0:2*N, :,:]*sen_msk # skip 1st frame # ref\n",
    "    print(xN.shape) # (2*N, RO, PE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c40420",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the start time\n",
    "st = time.time()\n",
    "\n",
    "########################################################## Image selection\n",
    "# for loop size\n",
    "if opt==0:\n",
    "  L = N\n",
    "else:\n",
    "  L = 1\n",
    "  NInd = 0 # If '0' don't select an individual image\n",
    "\n",
    "for el in range(L):\n",
    "  if opt==0:\n",
    "    NInd = el+1 # pick one image from N images\n",
    "    Ns   = 4 # Number of channels common to all images\n",
    "    Nz   = 0 # Number of image specific channels\n",
    "    Nout = 1 # Number of outputs, 1 or N (2*Nout = output channels) \n",
    "    Nin = Nz+Ns\n",
    "  elif opt==1: # Fix noise stack in with image specific output channels\n",
    "    Ns   = 4 # Number of channels common to all images\n",
    "    Nz   = 0 # Number of image specific channels\n",
    "    Nout = N # Number of outputs, 1 or N (2*Nout = output channels)\n",
    "    Nin = Nz+Ns\n",
    "  elif opt==2: # [Ns; Nz] in with a single output channel\n",
    "    Ns   = 3 # Number of channels common to all images #<--3\n",
    "    Nz   = 1 # Number of image specific channels #<--1\n",
    "    Nout = 1 # Number of outputs, 1 or N (2*Nout = output channels)\n",
    "    Nin = Nz+Ns\n",
    "  elif opt==3: # [Ns; Nz] in with image specific output channels\n",
    "    Ns   = 3 # Number of channels common to all images\n",
    "    Nz   = 1 # Number of image specific channels\n",
    "    Nout = N # Number of outputs, 1 or N (2*Nout = output channels)\n",
    "    Nin = Nz+Ns\n",
    "  elif opt==4: # [Ns + Nz] in with a single output channel\n",
    "    Ns   = 1 # Number of channels common to all images \n",
    "    Nz   = Ns # Number of image specific channels\n",
    "    Nout = 1 # Number of outputs, 1 or N (2*Nout = output channels)\n",
    "    Nin = Nz\n",
    "  elif opt==5: # H[Nz] in with image-specific first layer and single output channel \n",
    "    Ns   = 0 # Number of channels common to all images\n",
    "    Nz   = 2 # Number of image specific channels\n",
    "    Nout = 1 # Number of outputs, 1 or N (2*Nout = output channels)\n",
    "    Nin = Nz*N\n",
    "  elif opt==6: # H[Nz] in with image-specific first layer and image specific output channels\n",
    "    Ns   = 0 # Number of channels common to all images\n",
    "    Nz   = 2 # Number of image specific channels\n",
    "    Nout = N # Number of outputs, 1 or N (2*Nout = output channels)\n",
    "    Nin = Nz*N\n",
    "\n",
    "\n",
    "  if NInd==0: # process all images\n",
    "    if FS:\n",
    "      x  = xN\n",
    "    # y  = yN\n",
    "    #     yn = ynN\n",
    "    yu = yuN\n",
    "    msk= mskN\n",
    "    S = SN\n",
    "  elif NInd > 0: # Process only one image\n",
    "    N  = 1\n",
    "    x  = xN[(NInd-1)*2:(NInd-1)*2+2]\n",
    "    y  = yN[(NInd-1)*2:(NInd-1)*2+2]\n",
    "    yn = ynN[(NInd-1)*2:(NInd-1)*2+2]\n",
    "    yu = yuN[(NInd-1)*2:(NInd-1)*2+2]\n",
    "    msk= mskN[(NInd-1):NInd]\n",
    "\n",
    "\n",
    "\n",
    "  ########################################################## Network setup\n",
    "  pad = 'reflection' # 'zero'\n",
    "  mse = torch.nn.MSELoss().type(dtype)\n",
    "  mae = torch.nn.L1Loss().type(dtype)\n",
    "\n",
    "  INPUT = 'noise' # 'noise', 'meshgrid', or 'hybrid'\n",
    "\n",
    "  # ra: note, setting num_channels_skip[0] = 0 may improve performance\n",
    "  if opt==5 or opt==6: \n",
    "    net = skip(Nin, 2*Nout, # skip|skip_depth6|skip_depth4|skip_depth2|UNET|ResNet\n",
    "    num_channels_down = [2]+[LSz]*(NLy-1), #[2, 128, 128, 128, 128, 128]\n",
    "    # num_channels_down[0] = 2,\n",
    "    num_channels_up =   [LSz]*NLy, #[128, 128, 128, 128, 128, 128]\n",
    "    num_channels_skip = [LSz]*NLy, #[128, 128, 128, 128, 128, 128] \n",
    "    filter_size_up   = [3]*NLy, #[3, 3, 3, 3, 3, 3], \n",
    "    filter_size_down = [3]*NLy, #[3, 3, 3, 3, 3, 3],\n",
    "    filter_skip_size = 1, # kernel size for the filters along the skip connections\n",
    "    upsample_mode='nearest', \n",
    "    output_act = 0, # 0 for none, 1 for sigmoid, 2 for tanh\n",
    "    need_bias=True, \n",
    "    pad=pad, \n",
    "    act_fun='LeakyReLU').type(dtype) # need_sigmoid forces the out to between 0 and 1\n",
    "  else:\n",
    "    # RA: The number of layers = 2^layers < image size\n",
    "    # Five layers for sl64 and six for other applications\n",
    "    net = skip(Nin, 2*Nout, # skip|skip_depth6|skip_depth4|skip_depth2|UNET|ResNet\n",
    "    num_channels_down = [LSz]*NLy, #[128, 128, 128, 128, 128]\n",
    "    num_channels_up =   [LSz]*NLy, #[128, 128, 128, 128, 128]\n",
    "    num_channels_skip =  [LSz]*NLy, # [LSz]*NLy, #[128, 128, 128, 128, 128]  \n",
    "    filter_size_up   = [3]*NLy, #[3, 3, 3, 3, 3], \n",
    "    filter_size_down = [3]*NLy, #[3, 3, 3, 3, 3],\n",
    "    filter_skip_size = 1, # kernel size for the filters along the skip connections\n",
    "    upsample_mode='nearest', \n",
    "    output_act = 0, # 0 for none, 1 for sigmoid, 2 for tanh\n",
    "    need_bias=True, \n",
    "    pad=pad, \n",
    "    act_fun='LeakyReLU', dropout=p).type(dtype) # need_sigmoid forces the out to between 0 and 1\n",
    "\n",
    "\n",
    "  s  = sum(np.prod(list(p.size())) for p in net.parameters())\n",
    "  print ('Number of params: %d' % s)\n",
    "\n",
    "\n",
    "  ########################################################## Setup network inputs\n",
    "  # generate network input\n",
    "  if FS:\n",
    "    x_tor   = np_to_torch(x).type(dtype)\n",
    "  yu_tor  = np_to_torch(yu).type(dtype)\n",
    "#   yn_tor  = np_to_torch(yn).type(dtype)\n",
    "  msk_tor = np_to_torch(msk).type(dtype)\n",
    "  # x_avg_tor = np_to_torch(x_avg).type(dtype)\n",
    "  S_tor = np_to_torch(S).type(dtype)\n",
    "\n",
    "#   print(yu_tor.size())\n",
    "#   print(msk_tor.size())\n",
    "#   print(S_tor.size())\n",
    "\n",
    "  net = net.type(dtype)\n",
    "  if opt==5 or opt==6:\n",
    "    # net = net.type(dtype)\n",
    "    z0 = torch.zeros(1,Nz,n[0],n[1]).type(dtype) # all zeros\n",
    "    z = get_noise(Nz, INPUT, x.shape[1:], var=1/10).type(dtype) - 1/20\n",
    "    z_saved = torch.clone(z)   # ra: use clone so that z and z_saved don't point to the same location\n",
    "    z0_saved = torch.clone(z0)\n",
    "    # zs = get_noise(Nin-Nz, INPUT, x.shape[1:], var=1./10).type(dtype) - 1/20\n",
    "    # zs_saved = torch.clone(zs)  # ra: use clone so that zs and zs_saved don't point to the same location\n",
    "\n",
    "  elif opt==4:\n",
    "    z = torch.zeros(1,Nz*N,n[0],n[1]).type(dtype) # all zeros\n",
    "    zs = get_noise(Ns, INPUT, x.shape[1:], var=1./10).type(dtype) - 1/20\n",
    "    z_saved  = torch.clone(z)\n",
    "    zs_saved = torch.clone(zs)\n",
    "    z0 = torch.zeros(1,Nz*N,n[0],n[1]).type(dtype) # all zeros\n",
    "    z0_saved = torch.clone(z0)\n",
    "\n",
    "  else: \n",
    "    zs = get_noise(Nin-Nz, INPUT, yu.shape[2:], var=1./10).type(dtype) - 1/20\n",
    "    zs_saved = torch.clone(zs)  # ra: use clone so that zs and zs_saved don't point to the same location\n",
    "    \n",
    "    z0 = get_noise(Nz, INPUT, yu.shape[2:], var=1./10).type(dtype) - 1/20\n",
    "    z = torch.zeros(1,Nz*N,n[0],n[1]).type(dtype) # all zeros\n",
    "    for i in range(N):\n",
    "      z[:,i*Nz:(i+1)*Nz,:,:] = z_scale * (0.8*z0 + 0.2*(get_noise(Nz, INPUT, yu.shape[2:], var=1./10).type(dtype) - 1/20))\n",
    "    z0 = torch.tile(z0,[1,N,1,1])\n",
    "    z_saved  = torch.clone(z)   # ra: use clone so that z and z_saved don't point to the same location\n",
    "    z0_saved = torch.clone(z0)\n",
    "\n",
    "  # print(z.shape)\n",
    "\n",
    "  ### tuned DISCUS hyperparameters:\n",
    "  if R==2:\n",
    "    LR = 1.7e-4 #1.5e-4 #1.5 4, 1.5 1 # 1 4, 1 1 # 7 5, 7 0 # 5 5, 7 0\n",
    "    z_lamb0 = 1.5e1\n",
    "  elif R==3:\n",
    "    LR = 1e-4 #1.5 4, 1.5 1 # 1 4, 1 1 # 7 5, 7 0 # 5 5, 7 0\n",
    "    z_lamb0 = 1e1\n",
    "  elif R==4:\n",
    "    LR = 7e-5 #1.5 4, 1.5 1 # 1 4, 1 1 # 7 5, 7 0 # 5 5, 7 0\n",
    "    z_lamb0 = 7e0\n",
    "    if nt==16:\n",
    "      LR = 5e-5 #1.5 4, 1.5 1 # 1 4, 1 1 # 7 5, 7 0 # 5 5, 7 0\n",
    "      z_lamb0 = 5e0\n",
    "  else: # both retrospective R=5 and prospective R=5.069307\n",
    "    if nt==32:\n",
    "        LR = 1e-3#1e-4 #1.5 4, 1.5 1 # 1 4, 1 1 # 7 5, 7 0 # 5 5, 7 0\n",
    "        z_lamb0 = 1e1\n",
    "    elif nt==16:\n",
    "        LR = 5e-4#1e-4 #1.5 4, 1.5 1 # 1 4, 1 1 # 7 5, 7 0 # 5 5, 7 0\n",
    "        z_lamb0 = 7e0\n",
    "    else:\n",
    "        LR = 1e-4#1e-4 #1.5 4, 1.5 1 # 1 4, 1 1 # 7 5, 7 0 # 5 5, 7 0\n",
    "        z_lamb0 = 4e0\n",
    "\n",
    "\n",
    "\n",
    "  ########################################################## Main iterations\n",
    "  # from torch.nn.modules.loss import L1Loss\n",
    "  ii = 0\n",
    "  def closure():\n",
    "      global ii\n",
    "      # global running_loss\n",
    "      # global z_lamb\n",
    "      z_lamb = z_lamb0 #*(1 + 99 * ii/num_iter)\n",
    "      losses = torch.empty([N,1]).type(dtype)\n",
    "      xHat_tor = torch.empty(N,1,2*Nout, 1, n[0], n[1]).type(dtype)\n",
    "      reg_sig = reg_sig0*(1 - 0.9 * ii/num_iter)\n",
    "#       if ii<=num_iter/2:\n",
    "#         reg_sig = reg_sig0*0.9\n",
    "#       else:\n",
    "#         reg_sig = 0  \n",
    "     # reg_sig = reg_sig0 * (1 - ii/num_iter)\n",
    "      for i in range(N):\n",
    "        if opt==5 or opt==6:\n",
    "          xHat_tor[i,:,:,:,:] = net(torch.cat((torch.randn(1,Nz*i,n[0],n[1]).type(dtype) * reg_sig/(N**0.5), z + (torch.randn(1,Nz,n[0],n[1]).type(dtype) * reg_sig), torch.randn(1,N*Nz-Nz*(i+1),n[0],n[1]).type(dtype)* reg_sig/(N**0.5)), 1))\n",
    "        elif opt==4:\n",
    "          xHat_tor[i,:,:,:,:] = net(zs + z[:,i*Nz:(i+1)*Nz,:,:] + (torch.randn(1,Nz,n[0],n[1]).type(dtype) * reg_sig))\n",
    "        else: \n",
    "            ## opt = 2\n",
    "          xHat_tor[i,:,:,:,:,:] = torch.unsqueeze(net(torch.cat((zs + (torch.randn(1,Nin-Nz,n[0],n[1]).type(dtype) * reg_sig), z[:,i*Nz:(i+1)*Nz,:,:] + (torch.randn(1,Nz,n[0],n[1]).type(dtype) * reg_sig)), 1)), -3) #<-- change *1 to *4 AND <--zs to zs_saved\n",
    "          #xHat_tor[i,:,:,:,:,:] = torch.unsqueeze(net(torch.cat((zs, z[:,i*Nz:(i+1)*Nz,:,:] + (torch.randn(1,Nz,n[0],n[1]).type(dtype) * reg_sig)), 1)), -3) #<-- change *1 to *4 AND <--zs to zs_saved\n",
    "            # print(xHat_tor[i,:,:,:,:,:].size())\n",
    "          # xH = np.squeeze(torch_to_np(xHat_tor[i,:,:,:,:,:])) # (2, 160, 96)\n",
    "          # xHAbs = np.sqrt((xH[0,:,:])**2 + (xH[1,:,:])**2)\n",
    "          # plt.imshow(xHAbs, cmap=plt.cm.Greys_r)\n",
    "          # plt.show()\n",
    "          #print(xHat_tor.size())\n",
    "        if Nout==N:\n",
    "          xHatF_tor = fft2c_ra(xHat_tor[i,:,i*2:(i+1)*2,:,:], 'ortho')\n",
    "        elif Nout<N:\n",
    "            ## this case\n",
    "          xHatF_tor = fft2c_pra(multc(xHat_tor[i,:,:,:,:,:], S_tor), 'ortho')\n",
    "          # print(S_tor.size())\n",
    "          # print(xHatF_tor.size())\n",
    "          # xHF = torch_to_np(xHatF_tor) # (2, 8, 160, 96)\n",
    "          # xHFAbs = np.sqrt((xHF[0,:,:,:])**2 + (xHF[1,:,:,:])**2)\n",
    "          # fig = plt.figure(figsize=(16,5),facecolor='white', edgecolor=None)\n",
    "          # plt.imshow(np.reshape(np.transpose(xHFAbs,[1,0,2]), [n[0],n[1]*Nc]), cmap=plt.cm.Greys_r)\n",
    "          # plt.show()\n",
    "#         print(xHat_tor.size())\n",
    "#         print(xHatF_tor.size()) \n",
    "        losses[i] = mse(xHatF_tor*msk_tor[:,i:i+1,:,:,:],  yu_tor[:,i*2:(i+1)*2,:,:,:])\n",
    "        # print(msk_tor[:,i:i+1,:,:,:].size())\n",
    "        # print(yu_tor[:,i*2:(i+1)*2,:,:,:].size())\n",
    "        #losses[i] = mse(xHatF_tor,  yu_tor[:,i*2:(i+1)*2,:,:,:])\n",
    "        # yund = torch_to_np(yu_tor[:,i*2:(i+1)*2,:,:,:]) # (2, 8, 160, 96)\n",
    "        # yundAbs = np.sqrt((yund[0,:,:,:])**2 + (yund[1,:,:,:])**2)\n",
    "        # fig = plt.figure(figsize=(16,5),facecolor='white', edgecolor=None)\n",
    "        # plt.imshow(np.reshape(np.transpose(yundAbs,[1,0,2]), [n[0],n[1]*Nc]), cmap=plt.cm.Greys_r)\n",
    "        # plt.show()\n",
    "      if opt==2 or opt==3 or opt==4:\n",
    "        z_loss = z_lamb * torch.mean(torch.sqrt(torch.mean(torch.abs(z)**2, axis=1)+1e-6)) # img spec.\n",
    "        # z_loss = z_lamb*mae(z, z0_saved) # z_loss = mse(z,z0_saved) #<--default\n",
    "        # z_loss = z_lamb*mae(z, torch.tile(z[:,0:Nz,:,:],[1,N,1,1]))\n",
    "        # z_loss = z_lamb*mae(z, torch.tile(z[:,np.remainder(ii,N)*Nz:(np.remainder(ii,N)+1)*Nz,:,:],[1,N,1,1]))\n",
    "      else:\n",
    "        z_loss = 0\n",
    "      total_loss = sum(losses) + z_loss\n",
    "      total_loss.backward()\n",
    "      running_loss[0,ii] = sum(losses) + z_loss/z_lamb\n",
    "  \n",
    "      if FS:\n",
    "    ###\n",
    "        psn = np.zeros([N,1])\n",
    "\n",
    "        xHat = np.zeros((2*N,n[0],n[1]))\n",
    "        xHatS = np.zeros((2*N,n[0],n[1]))\n",
    "        xHatAbs = np.zeros((N,n[0],n[1]))\n",
    "        xAbs = np.zeros((N,n[0],n[1]))\n",
    "          \n",
    "        for i in range(N):\n",
    "          xHat[i*2:(i+1)*2,:,:] = np.squeeze(torch_to_np(xHat_tor[i,:,:,:,:,:]))\n",
    "          xHatS[i*2:(i+1)*2,:,:] = xHat[i*2:(i+1)*2,:,:] * sen_msk\n",
    "          xHatAbs[i:i+1,:,:] = np.sqrt((xHatS[i*2,:,:])**2 + (xHatS[i*2+1,:,:])**2)\n",
    "          xAbs[i:i+1,:,:] = np.sqrt((x[i*2,:,:])**2 + (x[i*2+1,:,:])**2)\n",
    "          psn[i] = psnr(xAbs[i,RO_offset:-RO_offset,:], xHatAbs[i,RO_offset:-RO_offset,:], data_range = xHatAbs[i,RO_offset:-RO_offset,:].max() - xHatAbs[i,RO_offset:-RO_offset,:].min())\n",
    "          \n",
    "        running_psn[0,ii] = np.mean(psn)\n",
    "    ###\n",
    "      # print ('Iteration %05d    Loss %f' % (i, total_loss.item()), '\\r', end='')\n",
    "      if  PLOT and ( ii % show_every == 0):\n",
    "        if FS:\n",
    "          nmse = np.zeros([N,1])\n",
    "          ssm = np.zeros([N,1])\n",
    "          psn = np.zeros([N,1])\n",
    "          xAbs = np.zeros((N,n[0],n[1]))\n",
    "          errMap = np.zeros((2*N,n[0],n[1]))\n",
    "        \n",
    "        xHat = np.zeros((2*N,n[0],n[1]))\n",
    "        xHatS = np.zeros((2*N,n[0],n[1]))\n",
    "        xHatAbs = np.zeros((N,n[0],n[1]))\n",
    "\n",
    "        # xHatF = np.zeros((2*N,n[0],n[1]))\n",
    "        # xHatFAbs = np.zeros((N,n[0],n[1]))\n",
    "        # yAbs = np.zeros((N,n[0],n[1]))\n",
    "        # errFMap = np.zeros((N,n[0],n[1]))\n",
    "        for i in range(N):\n",
    "          if Nout==N:\n",
    "            xHat[i*2:(i+1)*2,:,:] = torch_to_np(xHat_tor[i, :, i*2:(i+1)*2,:,:,:])\n",
    "            xHatAbs[i:i+1,:,:] = np.sqrt((xHat[i*2,:,:])**2 + (xHat[i*2+1,:,:])**2)\n",
    "#             xAbs[i:i+1,:,:] = np.sqrt((x[i*2,:,:])**2 + (x[i*2+1,:,:])**2)\n",
    "          elif Nout<N:\n",
    "            # this case\n",
    "            xHat[i*2:(i+1)*2,:,:] = np.squeeze(torch_to_np(xHat_tor[i,:,:,:,:,:]))\n",
    "            xHatS[i*2:(i+1)*2,:,:] = xHat[i*2:(i+1)*2,:,:] * sen_msk\n",
    "            xHatAbs[i:i+1,:,:] = np.sqrt((xHatS[i*2,:,:])**2 + (xHatS[i*2+1,:,:])**2)\n",
    "            if FS:\n",
    "              xAbs[i:i+1,:,:] = np.sqrt((x[i*2,:,:])**2 + (x[i*2+1,:,:])**2)\n",
    "            # xHatF[i*2:(i+1)*2,:,:] = np.squeeze(torch_to_np(fft2c_pra(np_to_torch(np.expand_dims(xHatS[i*2:(i+1)*2,:,:], axis=1)),'ortho')))\n",
    "            # xHatFAbs[i:i+1,:,:] = np.sqrt((xHatF[i*2,:,:])**2 + (xHatF[i*2+1,:,:])**2)\n",
    "            # yAbs[i:i+1,:,:] = np.sqrt((y[i*2,:,:])**2 + (y[i*2+1,:,:])**2)\n",
    "          if FS:\n",
    "            nmse[i] =  np.mean((x[i*2:(i+1)*2,RO_offset:-RO_offset,:]-xHatS[i*2:(i+1)*2,RO_offset:-RO_offset,:])**2) / np.mean((x[i*2:(i+1)*2,RO_offset:-RO_offset,:])**2)\n",
    "            ssm[i] = ssim(xHatAbs[i,RO_offset:-RO_offset,:], xAbs[i,RO_offset:-RO_offset,:], data_range = xHatAbs[i,RO_offset:-RO_offset,:].max() - xHatAbs[i,RO_offset:-RO_offset,:].min()) # xHatL1Abs[i:i+1,:,:].max() - xHatL1Abs[i:i+1,:,:].min() \n",
    "            psn[i] = psnr(xAbs[i,RO_offset:-RO_offset,:], xHatAbs[i,RO_offset:-RO_offset,:], data_range = xHatAbs[i,RO_offset:-RO_offset,:].max() - xHatAbs[i,RO_offset:-RO_offset,:].min())\n",
    "            errMap[i*2:(i+1)*2,:,:] = x[i*2:(i+1)*2,:,:]-xHatS[i*2:(i+1)*2,:,:]\n",
    "            # errFMap[i,:,:] = np.sqrt(np.sum(np.abs(y[i*2:(i+1)*2,:,:]-xHatF[i*2:(i+1)*2,:,:])**2, axis=0))\n",
    "        ## Printing and Plotting begins here...\n",
    "        print('\\nIteration: %1.3d,' %(ii+1), 'Loss x 1e4: %1.2f,' %(running_loss[0,ii]*1e4))\n",
    "        print('Individual losses x 1e4: ',', '.join('%1.3f' % (losses[j]*1e4) for j in range(len(losses))))\n",
    "\n",
    "        fig = plt.figure(figsize=(16,5),facecolor='white', edgecolor=None)\n",
    "        plt.imshow(np.reshape(np.transpose(xHat**g,[1,0,2]), [n[0],n[1]*2*N]), cmap=mps[0]) # use a specific color map\n",
    "        plt.title(\"xHat\")\n",
    "        plt.show()\n",
    "\n",
    "        if FS:\n",
    "          fig = plt.figure(figsize=(16,5),facecolor='white', edgecolor=None)\n",
    "          plt.imshow(np.reshape(np.transpose(errMap,[1,0,2]), [n[0],n[1]*2*N]), vmin=-0.1, vmax=0.1, cmap=plt.cm.Greys_r) # use a specific color map\n",
    "          plt.title(\"errxMap\")\n",
    "          plt.show()\n",
    "          print('Mean nmse: %1.2f,' %(10*np.log10(np.mean(nmse))), 'nmse: ',', '.join('%1.2f' % (10*np.log10(nmse[j])) for j in range(len(nmse)))) \n",
    "          print('Mean ssim: %1.3f,' %(np.mean(ssm)), 'ssim: ',', '.join('%1.3f' % (ssm[j]) for j in range(len(ssm)))) \n",
    "          print('Mean psnr: %1.3f,' %(np.mean(psn)), 'psnr: ',', '.join('%1.3f' % (psn[j]) for j in range(len(psn)))) \n",
    "\n",
    "        # fig = plt.figure(figsize=(16,5),facecolor='white', edgecolor=None)\n",
    "        # plt.imshow(np.reshape(np.transpose(xHatFAbs**gm[1],[1,0,2]), [n[0],n[1]*N]), cmap=mps[1]) # use a specific color map\n",
    "        # plt.title(\"xHatFAbs\")\n",
    "        # plt.show()\n",
    "          \n",
    "        # fig = plt.figure(figsize=(16,5),facecolor='white', edgecolor=None)\n",
    "        # plt.imshow(np.reshape(np.transpose(yAbs**gm[1],[1,0,2]), [n[0],n[1]*N]), cmap=mps[1]) # use a specific color map\n",
    "        # plt.title(\"yAbs\")\n",
    "        # plt.show()\n",
    "          \n",
    "        # fig = plt.figure(figsize=(16,5),facecolor='white', edgecolor=None)\n",
    "        # plt.imshow(np.reshape(np.transpose(errFMap**gm[1],[1,0,2]), [n[0],n[1]*N]), cmap=plt.cm.Greys_r) # use a specific color map\n",
    "        # plt.title(\"errFMap\")\n",
    "        # plt.show() \n",
    "\n",
    "        fig = plt.figure(figsize=(16,5),facecolor='white', edgecolor=None)\n",
    "        if Nz == 0:\n",
    "          plt.imshow(np.reshape(np.transpose(np.concatenate((xAbs[0:1,:,:]**gm[0], xHatAbs[0:1,:,:]**gm[0]), axis=2),[1,0,2]), [n[0],n[1]*2]), vmin=0, vmax=0.7, cmap=plt.cm.Greys_r) # use a specific color map\n",
    "          plt.show() \n",
    "            \n",
    "        else: # opt=2\n",
    "          zNp = torch_to_np(z)\n",
    "          if FS:\n",
    "            plt.imshow(np.reshape(np.transpose(np.concatenate((xAbs[0:1,:,:]**gm[0], xHatAbs[0:1,:,:]**gm[0], 100*zNp[0:1,:,:]), axis=2),[1,0,2]), [n[0],n[1]*3]), cmap=plt.cm.Greys_r) # use a specific color map\n",
    "            plt.title(\"xAbs, xHatAbs, z (i=1)\")\n",
    "            plt.show()\n",
    "          else:\n",
    "            plt.imshow(np.reshape(np.transpose(np.concatenate((xHatAbs[0:1,:,:]**g, 10*zNp[0:1,:,:]), axis=2),[1,0,2]), [n[0],n[1]*2]), cmap=plt.cm.Greys_r) # use a specific color map\n",
    "            plt.title(\"xHatAbs, z (i=1)\")\n",
    "            plt.show()\n",
    "      ii += 1\n",
    "      return total_loss\n",
    "\n",
    "  running_loss = torch.empty([1,num_iter]).type(dtype)\n",
    "  if FS:\n",
    "    running_psn = torch.empty([1,num_iter]).type(dtype)\n",
    "\n",
    "  print('=====optimizing over network and input=====')\n",
    "  OPT_OVER = 'net,input'\n",
    "  # z_lamb0 = 8*2e0 #<-- default: 5e1 for mse, 2e0 for mae, and 2e0 for group sparsity\n",
    "  # WtD = 0*1e-6 # weight decay\n",
    "  if Nz != 0 and Ns != 0:\n",
    "    p = get_params(OPT_OVER, net, ([z,zs])) #<-- ra: remove zs\n",
    "  if Nz != 0 and Ns == 0:\n",
    "    p = get_params(OPT_OVER, net, ([z])) #<-- ra: remove zs\n",
    "  if Ns != 0 and Nz == 0:\n",
    "    p = get_params(OPT_OVER, net, ([zs])) #<-- ra: remove zs\n",
    "  optimize('adam', p, closure, LR, num_iter, WtD)\n",
    "\n",
    "  fig = plt.figure(figsize=(16,5),facecolor='white', edgecolor=None)\n",
    "  plt.plot(np.log10(torch_to_np(running_loss)))\n",
    "  plt.xlabel(\"No. of iterations\")\n",
    "  plt.ylabel(\"Total loss\")\n",
    "  plt.show()\n",
    "    \n",
    "  if FS:\n",
    "    fig = plt.figure(figsize=(16,5),facecolor='white', edgecolor=None)\n",
    "    plt.plot(torch_to_np(running_psn))\n",
    "    plt.xlabel(\"No. of iterations\")\n",
    "    plt.ylabel(\"PSNR\")\n",
    "    plt.show()\n",
    "    print(\"Max PSNR: \", np.max(torch_to_np(running_psn)), \"at Iteration: \", np.argmax(torch_to_np(running_psn)), \"/\", num_iter)\n",
    "\n",
    "\n",
    "  ########################################################## Display and saving\n",
    "  xHat_tor = torch.empty(N,1,2*Nout, 1, n[0], n[1]).type(dtype)\n",
    "  for i in range(N):\n",
    "    if opt==5 or opt==6:\n",
    "      xHat_tor[i,:,:,:,:] = net(torch.cat((torch.zeros(1,Nz*i,n[0],n[1]).type(dtype), z , torch.zeros(1,N*Nz-Nz*(i+1),n[0],n[1]).type(dtype)), 1))\n",
    "    elif opt==4:\n",
    "      xHat_tor[i,:,:,:,:] = net(1*zs + 1*z[:,i*Nz:(i+1)*Nz,:,:])\n",
    "    else: \n",
    "        # this case\n",
    "      xHat_tor[i,:,:,:,:,:] = torch.unsqueeze(net(torch.cat((1*zs, 1*z[:,i*Nz:(i+1)*Nz,:,:]), 1)), -3)\n",
    "\n",
    "  if FS:\n",
    "    nmse = np.zeros([N,1])\n",
    "    ssm = np.zeros([N,1])\n",
    "    psn = np.zeros([N,1])\n",
    "    xAbs = np.zeros((N,n[0],n[1]))\n",
    "    errMap = np.zeros((N,n[0],n[1]))\n",
    "\n",
    "  xHat = np.zeros((2*N,n[0],n[1]))\n",
    "  xHatS = np.zeros((2*N,n[0],n[1]))\n",
    "  xHatAbs = np.zeros((N,n[0],n[1]))\n",
    "  xHatF = np.zeros((2*N,n[0],n[1]))\n",
    "  xHatFAbs = np.zeros((N,n[0],n[1]))\n",
    "  # yAbs = np.zeros((N,n[0],n[1]))\n",
    "  # errFMap = np.zeros((N,n[0],n[1]))\n",
    "\n",
    "  for i in range(N):\n",
    "    if Nout==N:\n",
    "      xHat[i*2:(i+1)*2,:,:] = torch_to_np(xHat_tor[i, :, i*2:(i+1)*2,:,:,:])\n",
    "      xHatAbs[i:i+1,:,:] = np.sqrt((xHat[i*2,:,:])**2 + (xHat[i*2+1,:,:])**2)\n",
    "      # xHatF[i*2:(i+1)*2,:,:] = torch_to_np(fft2c_ra(xHat_tor[i, :, i*2:(i+1)*2,:,:,:],'ortho'))\n",
    "      # xHatFAbs[i:i+1,:,:] = np.sqrt((xHatF[i*2,:,:])**2 + (xHatF[i*2+1,:,:])**2)\n",
    "      xAbs[i:i+1,:,:] = np.sqrt((x[i*2,:,:])**2 + (x[i*2+1,:,:])**2)\n",
    "    elif Nout<N:\n",
    "        ## this case\n",
    "      xHat[i*2:(i+1)*2,:,:] = np.squeeze(torch_to_np(xHat_tor[i,:,:,:,:,:]))\n",
    "      xHatS[i*2:(i+1)*2,:,:] = xHat[i*2:(i+1)*2,:,:] * sen_msk\n",
    "      xHatAbs[i:i+1,:,:] = np.sqrt((xHatS[i*2,:,:])**2 + (xHatS[i*2+1,:,:])**2)\n",
    "    \n",
    "      # xHatF[i*2:(i+1)*2,:,:] = np.squeeze(torch_to_np(fft2c_pra(np_to_torch(np.expand_dims(xHatS[i*2:(i+1)*2,:,:], axis=1)),'ortho')))\n",
    "      # xHatFAbs[i:i+1,:,:] = np.sqrt((xHatF[i*2,:,:])**2 + (xHatF[i*2+1,:,:])**2)\n",
    "      if FS:\n",
    "        xAbs[i:i+1,:,:] = np.sqrt((x[i*2,:,:])**2 + (x[i*2+1,:,:])**2)\n",
    "      # yAbs[i:i+1,:,:] = np.sqrt((y[i*2,:,:])**2 + (y[i*2+1,:,:])**2)\n",
    "\n",
    "    if FS:\n",
    "      nmse[i] =  np.mean((x[i*2:(i+1)*2,RO_offset:-RO_offset,:]-xHatS[i*2:(i+1)*2,RO_offset:-RO_offset,:])**2) / np.mean((x[i*2:(i+1)*2,RO_offset:-RO_offset,:])**2)\n",
    "      ssm[i] = ssim(xHatAbs[i,RO_offset:-RO_offset,:], xAbs[i,RO_offset:-RO_offset,:], data_range = xHatAbs[i,RO_offset:-RO_offset,:].max() - xHatAbs[i,RO_offset:-RO_offset,:].min()) # xHatL1Abs[i:i+1,:,:].max() - xHatL1Abs[i:i+1,:,:].min()      \n",
    "      psn[i] = psnr(xAbs[i,RO_offset:-RO_offset,:], xHatAbs[i,RO_offset:-RO_offset,:], data_range = xHatAbs[i,RO_offset:-RO_offset,:].max() - xHatAbs[i,RO_offset:-RO_offset,:].min())\n",
    "      errMap[i,:,:] = np.sqrt(np.sum(np.abs(x[i*2:(i+1)*2,:,:]-xHatS[i*2:(i+1)*2,:,:])**2, axis=0))\n",
    "      # errFMap[i,:,:] = np.sqrt(np.sum(np.abs(y[i*2:(i+1)*2,:,:]-xHatF[i*2:(i+1)*2,:,:])**2, axis=0))\n",
    "  \n",
    "## printing and plotting after iterations:\n",
    "  if FS:\n",
    "    print('Mean nmse: %1.2f,' %(10*np.log10(np.mean(nmse))), 'nmse: ',', '.join('%1.2f' % (10*np.log10(nmse[j])) for j in range(len(nmse)))) \n",
    "    print('Mean ssim: %1.3f,' %(np.mean(ssm)), 'ssim: ',', '.join('%1.3f' % (ssm[j]) for j in range(len(ssm)))) \n",
    "    print('Mean psnr: %1.3f,' %(np.mean(psn)), 'psnr: ',', '.join('%1.3f' % (psn[j]) for j in range(len(psn)))) \n",
    "\n",
    "  # fig = plt.figure(figsize=(16,5),facecolor='white', edgecolor=None)\n",
    "  # plt.imshow(np.reshape(np.transpose(xHatFAbs**gm[1],[1,0,2]), [n[0],n[1]*N]), cmap=mps[1]) # use a specific color map\n",
    "  # plt.title(\"xHatFAbs\")\n",
    "  # plt.show()\n",
    "    \n",
    "  # fig = plt.figure(figsize=(16,5),facecolor='white', edgecolor=None)\n",
    "  # plt.imshow(np.reshape(np.transpose(yAbs**gm[1],[1,0,2]), [n[0],n[1]*N]), cmap=mps[1]) # use a specific color map\n",
    "  # plt.title(\"yAbs\")\n",
    "  # plt.show()\n",
    "    \n",
    "  # fig = plt.figure(figsize=(16,5),facecolor='white', edgecolor=None)\n",
    "  # plt.imshow(np.reshape(np.transpose(errFMap**gm[1],[1,0,2]), [n[0],n[1]*N]), cmap=plt.cm.Greys_r) # use a specific color map\n",
    "  # plt.title(\"errFMap\")\n",
    "  # plt.show() \n",
    "    \n",
    "    fig = plt.figure(figsize=(16,5),facecolor='white', edgecolor=None)\n",
    "    plt.imshow(np.reshape(np.transpose(xAbs**g,[1,0,2]), [n[0],n[1]*N]), cmap=mps[0]) # use a specific color map\n",
    "    plt.title(\"xAbs\")\n",
    "    plt.show()\n",
    "      \n",
    "    fig = plt.figure(figsize=(16,5),facecolor='white', edgecolor=None)\n",
    "    plt.imshow(np.reshape(np.transpose(errMap,[1,0,2]), [n[0],n[1]*N]), cmap=plt.cm.Greys_r) # use a specific color map\n",
    "    plt.title(\"errxMap\")\n",
    "    plt.show()\n",
    "\n",
    "  fig = plt.figure(figsize=(16,5),facecolor='white', edgecolor=None)\n",
    "  plt.imshow(np.reshape(np.transpose(xHatAbs**g,[1,0,2]), [n[0],n[1]*N]), cmap=mps[0]) # use a specific color map\n",
    "  plt.title(\"xHatAbs\")\n",
    "  plt.show()\n",
    "  # model = torch.load(data_path + 'model_'+'opt_%d_N_%d_Ind_%d' % (opt, N, NInd))\n",
    "  # model.eval()\n",
    "  ########################################################## Display/read the weights\n",
    "  # # Print model's state_dict\n",
    "  # print(\"Model's state_dict:\")\n",
    "  # for param_tensor in net.state_dict():\n",
    "  #     print(param_tensor, \"\\t\", net.state_dict()[param_tensor].size())\n",
    "\n",
    "  # # Print optimizer's state_dict\n",
    "  # print(\"Optimizer's state_dict:\")\n",
    "  # for var_name in optimizer.state_dict():\n",
    "  #     print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n",
    "\n",
    "  # print(net) # this gives the structure of the network\n",
    "  L = 4 # read thie weights of this layer\n",
    "  i=0\n",
    "  for param in net.parameters():\n",
    "    wt = param.data\n",
    "    if i == L:\n",
    "      break\n",
    "    i=i+1\n",
    "\n",
    "  print(wt.shape)\n",
    "  print(wt.view(-1)) # this view(-1) reshpaes into a list\n",
    "  # print(zs.shape)\n",
    "  ########################################################## Display code vectors\n",
    "  if Ns !=0: # common z for all images (Ns = 3 for opt 2)\n",
    "    for i in range(Ns):\n",
    "        fig = plt.figure(figsize=(16,5),facecolor='white', edgecolor=None)\n",
    "        plt.imshow(np.concatenate((torch_to_np(zs_saved[:,i,:,:]), torch_to_np(zs[:,i,:,:]), torch_to_np(zs[:,i,:,:]-zs_saved[:,i,:,:])), axis=1), cmap=plt.cm.Greys_r) # use a specific color map\n",
    "        plt.title(\"Common Code Vector: \"+str(i+1)+\" (zs_init, zs, diff)\")\n",
    "        plt.show()\n",
    "        print(\"max diff. \",np.max(torch_to_np(zs[:,i,:,:]-zs_saved[:,i,:,:])))  \n",
    "\n",
    "  if Nz !=0: # 1 z for each image (Nz = 1 for opt 2)\n",
    "    for i in range(N*Nz):\n",
    "      fig = plt.figure(figsize=(16,5),facecolor='white', edgecolor=None)\n",
    "      plt.imshow(np.concatenate((torch_to_np(z_saved[:,i,:,:]), torch_to_np(z[:,i,:,:]), torch_to_np(z[:,i,:,:]-z_saved[:,i,:,:])) , axis=1), cmap=plt.cm.Greys_r) # use a specific color map\n",
    "      plt.title(\"Image-Specific Code Vector: \"+str(i+1)+\" (z_init, z, diff)\")\n",
    "      plt.show()\n",
    "      print(\"min: \",torch.min(z[:,i,:,:]))\n",
    "      print(\"max: \",torch.max(z[:,i,:,:]))\n",
    "  \n",
    "    \n",
    "# get the end time\n",
    "et = time.time()\n",
    "\n",
    "# get the execution time\n",
    "elapsed_time = et - st\n",
    "print('Execution time:', elapsed_time/60, 'minutes')\n",
    "\n",
    "# sv=1\n",
    "# if FS:\n",
    "  # if sv==1: np.save(data_path_r + 'xRefDISCUS_'+'N_%d_LR_%f_zlamb_%f' % (N, LR, z_lamb0) + '_R_%f'%R + '_numIters_%d'%num_iter + '.npy', x)\n",
    "  # if sv==1: np.save(data_path_r + 'nmseDISCUS_'+'N_%d_LR_%f_zlamb_%f' % (N, LR, z_lamb0) + '_R_%f'%R + '_numIters_%d'%num_iter + '.npy', nmse)\n",
    "  # if sv==1: np.save(data_path_r + 'ssimDISCUS_'+'N_%d_LR_%f_zlamb_%f' % (N, LR, z_lamb0) + '_R_%f'%R + '_numIters_%d'%num_iter + '.npy', ssm)\n",
    "  # if sv==1: np.save(data_path_r + 'psnrDISCUS_'+'N_%d_LR_%f_zlamb_%f' % (N, LR, z_lamb0) + '_R_%f'%R + '_numIters_%d'%num_iter + '.npy', psn)\n",
    "  # if sv==1: np.save(data_path_r + 'PSNRtracking_DISCUS_'+'N_%d_LR_%f_zlamb_%f' % (N, LR, z_lamb0) + '_R_%f'%R + '_numIters_%d'%num_iter + '.npy', torch_to_np(running_psn))\n",
    "\n",
    "if sv==1: np.save(data_path_r + 'xHatDISCUS_'+'N_%d_LR_%f_zlamb_%f' % (N, LR, z_lamb0) + '_R_%f'%R + '_numIters_%d'%num_iter + '.npy', xHatS)\n",
    "\n",
    "# if Nz!=0:\n",
    "#   if sv==1: np.save(data_path_r + 'zDISCUS_'+'N_%d_LR_%f_zlamb_%f' % (N, LR, z_lamb0) + '_R_%f'%R + '_numIters_%d'%num_iter + '.npy', torch_to_np(z))\n",
    "# if Ns!=0: \n",
    "#   if sv==1: np.save(data_path_r + 'zsDISCUS_'+'N_%d_LR_%f_zlamb_%f' % (N, LR, z_lamb0) + '_R_%f'%R + '_numIters_%d'%num_iter + '.npy', torch_to_np(zs))\n",
    "\n",
    "# if sv==1: np.save(data_path_r + 'LOSStracking_DISCUS_'+'N_%d_LR_%f_zlamb_%f' % (N, LR, z_lamb0) + '_R_%f'%R + '_numIters_%d'%num_iter + '.npy', torch_to_np(running_loss))\n",
    "\n",
    "# if sv==1: torch.save(net, data_path_r + 'model_DISCUS_'+'N_%d_LR_%f_zlamb_%f' % (N, LR, z_lamb0) + '_R_%f'%R + '_numIters_%d'%num_iter)\n",
    "# # if sv==1: torch.save(mlp, data_path + 'model_'+'opt_%d_N_%d_Ind_%d' % (opt, N, NInd))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DISCUS-Sultan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "552489c190a5836f1d6d306c55383d234ab7e9c654141c6542f91293c67cd02a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
