{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xzu7y5dmBvaW"
   },
   "source": [
    "# DISCUS Implementation for Shepp-Logan Phantom: Study-I\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X10QTe67qp8N"
   },
   "source": [
    "### Load utilitiues and libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7749,
     "status": "ok",
     "timestamp": 1669922783192,
     "user": {
      "displayName": "Rizwan Ahmad",
      "userId": "01148704056318093914"
     },
     "user_tz": 300
    },
    "id": "UgUlxSofWlue",
    "outputId": "2b4205de-867c-4421-ce97-f22f5bf5be54"
   },
   "outputs": [],
   "source": [
    "########################################################## Import libraries\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import numpy as np\n",
    "import pickle\n",
    "# from models.resnet import ResNet\n",
    "# from models.unet import UNet\n",
    "from models.skip import skip\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "from scipy.io import savemat, loadmat\n",
    "\n",
    "\n",
    "\n",
    "########################################################## from utils.inpainting_utils import *\n",
    "from utils_SL.common_utils import *\n",
    "from utils_SL.fftc import * # ra: added the pytorch fft routine from fastmri\n",
    "\n",
    "import os# os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark =True\n",
    "dtype = torch.cuda.FloatTensor\n",
    "\n",
    "PLOT = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check available hardware:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check available hardware:\n",
    "def list_cuda_devices():\n",
    "    if torch.cuda.is_available():\n",
    "        num_devices = torch.cuda.device_count()\n",
    "        print(f\"Number of available CUDA devices: {num_devices}\")\n",
    "        for i in range(num_devices):\n",
    "            print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "            print(f\"  Memory Allocated: {torch.cuda.memory_allocated(i) / 1024 ** 2:.2f} MB\")\n",
    "            print(f\"  Memory Cached: {torch.cuda.memory_reserved(i) / 1024 ** 2:.2f} MB\")\n",
    "            print(f\"  Total Memory: {torch.cuda.get_device_properties(i).total_memory / 1024 ** 2:.2f} MB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available.\")\n",
    "\n",
    "list_cuda_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.cuda.device_count()) # restart shell if it doesnt show 2\n",
    "# device=0\n",
    "# torch.cuda.set_device(device) # 0/1\n",
    "# print(torch.cuda.current_device())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f23qT96ZWrmp"
   },
   "source": [
    "## Set study parameters and select dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################## Adjust parameters\n",
    "data_path = '../data/SL-ph/'\n",
    "R = 2  # net acceleration rate\n",
    "gm = 1  # gamma correction for display\n",
    "N = 64  # number of repetitions = frames\n",
    "\n",
    "# select series:\n",
    "# A list of three series: [\"rotation\", \"translation\", \"both\"]\n",
    "series = 'both' \n",
    "\n",
    "\n",
    "sv = 1\n",
    "data_path_r = data_path + series + \"/\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load simulated data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yN = np.load(data_path_r + 'y_N_%d_R_%d' % (N, R) + '.npy')\n",
    "ynN = np.load(data_path_r + 'yn_N_%d_R_%d' % (N, R) + '.npy')\n",
    "yuN = np.load(data_path_r + 'yu_N_%d_R_%d' % (N, R) + '.npy')\n",
    "mskN = np.load(data_path_r + 'mask_R_%fN_%d_phantom' % (R, N) + '.npy')\n",
    "xN = np.load(data_path_r + 'xRef_N_%d' % N + '.npy')\n",
    "n = xN.shape[1:]\n",
    "print(\"Image size: \", n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CS-Recon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csRe = 1\n",
    "tau = 1e-2 # regularization strength\n",
    "\n",
    "########################################################## CS Recon\n",
    "\n",
    "if csRe==1:\n",
    "  # N=4\n",
    "  \n",
    "  # Running ADMM L2-L1\n",
    "  nmse = np.zeros([N,1])\n",
    "  ssm = np.zeros([N,1])\n",
    "  xHat = np.zeros((2*N,n[0],n[1]))\n",
    "  xHatAbs = np.zeros((N,n[0],n[1]))\n",
    "  xAbs = np.zeros((N,n[0],n[1]))\n",
    "  errMap = np.zeros((2*N,n[0],n[1]))\n",
    "\n",
    "  for i in range(N):\n",
    "    print('Slice: %2d' %(i+1), 'out of %2d' %N)\n",
    "    x0 = (np.zeros(n)).astype(complex)\n",
    "    y = yuN[2*i,:,:] + 1j*yuN[2*i+1,:,:]\n",
    "    msk = mskN[i, :,:]\n",
    "    mu = 1e-1 # lagrangian parameter\n",
    "    nIter = [50, 5] # outer and inner iterations\n",
    "    ss = 0.9 # step size\n",
    "\n",
    "    [xTmp,loss] = admm_l1(x0, y, msk, nIter, ss, mu, tau)\n",
    "    xHat[2*i,:,:]   = np.real(xTmp)\n",
    "    xHat[2*i+1,:,:] = np.imag(xTmp)\n",
    "    xHatAbs[i:i+1,:,:] = np.sqrt((xHat[i*2,:,:])**2 + (xHat[i*2+1,:,:])**2)\n",
    "\n",
    "    xAbs[i:i+1,:,:] = np.sqrt((xN[i*2,:,:])**2 + (xN[i*2+1,:,:])**2)\n",
    "    nmse[i] =  np.mean((xN[i*2:(i+1)*2,:,:]-xHat[i*2:(i+1)*2,:,:])**2) / np.mean((xN[i*2:(i+1)*2,:,:])**2)\n",
    "    ssm[i] = ssim(xHatAbs[i,:,:], xAbs[i,:,:], data_range = xHatAbs[i,:,:].max() - xHatAbs[i,:,:].min()) # xHatL1Abs[i:i+1,:,:].max() - xHatL1Abs[i:i+1,:,:].min()\n",
    "    errMap[i*2:(i+1)*2,:,:] = xN[i*2:(i+1)*2,:,:]-xHat[i*2:(i+1)*2,:,:]\n",
    "\n",
    "  fig = plt.figure(figsize=(16,5),facecolor='white', edgecolor=None)\n",
    "  plt.plot(np.log10(loss))\n",
    "  plt.xlabel(\"No. of iterations\")\n",
    "  plt.ylabel(\"Total loss\")\n",
    "  plt.show()\n",
    "          \n",
    "  print('Mean nmse: %1.2f,' %(10*np.log10(np.mean(nmse))), 'nmse: ',', '.join('%1.2f' % (10*np.log10(nmse[j])) for j in range(len(nmse)))) \n",
    "  print('Mean ssim: %1.3f,' %(np.mean(ssm)), 'ssim: ',', '.join('%1.3f' % (ssm[j]) for j in range(len(ssm)))) \n",
    "  fig = plt.figure(figsize=(16,5),facecolor='white', edgecolor=None)\n",
    "  plt.imshow(np.reshape(np.transpose(xHat,[1,0,2]), [n[0],n[1]*2*N]), vmin=-1, vmax=1, cmap=plt.cm.Greys_r) # use a specific color map\n",
    "  plt.show()\n",
    "\n",
    "  fig = plt.figure(figsize=(16,5),facecolor='white', edgecolor=None)\n",
    "  plt.imshow(np.reshape(np.transpose(errMap,[1,0,2]), [n[0],n[1]*2*N]), vmin=-0.1, vmax=0.1, cmap=plt.cm.Greys_r) # use a specific color map\n",
    "  plt.show()\n",
    "\n",
    "  fig = plt.figure(figsize=(16,5),facecolor='white', edgecolor=None)\n",
    "  plt.imshow(np.reshape(np.transpose(np.concatenate((xAbs[0:1,:,:]**gm, xHatAbs[0:1,:,:]**gm), axis=2),[1,0,2]), [n[0],n[1]*2]), vmin=0, vmax=0.7, cmap=plt.cm.Greys_r) # use a specific color map\n",
    "  plt.show()\n",
    "  # sv=1\n",
    "  if sv==1: \n",
    "    np.save(data_path_r + 'xRefL1_'+'N_%d' % N + '.npy', xN)\n",
    "    np.save(data_path_r + 'xHatL1_'+'N_%d' % N + '.npy', xHat)\n",
    "    # np.save(data_path_r + 'nmseL1_'+'N_%d' % N + '.npy', nmse)\n",
    "    # np.save(data_path_r + 'ssimL1_'+'N_%d' % N + '.npy', ssm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.max(takeMag(yuN)))\n",
    "# print(np.max(takeMag(xN)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N=64\n",
    "\n",
    "num_iter = 12000 # 15000 # number of iterations\n",
    "z_lamb0 = 4.3e2\n",
    "reg_sig0 = 0.005\n",
    "show_every = 1000\n",
    "z_sc = 0.1\n",
    "LR = 1e-4\n",
    "\n",
    "# sv=0\n",
    "# reg_sig0 = 0.03 # noise regularization\n",
    "\n",
    "WtD = 2*1e-6 # weight decay\n",
    "opt = 2 # select the flavor of the algorithm\n",
    "# LR  = 1e-3 # learning rate, original: 1e-2\n",
    "LSz = 128 # Number of channels in hidden layers\n",
    "NLy=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WmZ9bdcBeYJX"
   },
   "source": [
    "### DIP and DISCUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1d6_lMWZUidO1lLdrtfrFwF5Bq6sD4W_L"
    },
    "id": "8TYvqxXieYTQ",
    "outputId": "c3a52509-4c5e-4de9-ad13-2ff4d91274bd",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get the start time\n",
    "st = time.time()\n",
    "\n",
    "########################################################## Image selection\n",
    "# for loop size\n",
    "if opt==0:\n",
    "  L = N\n",
    "else:\n",
    "  L = 1\n",
    "  NInd = 0 # If '0' don't select an individual image\n",
    "\n",
    "for el in range(L):\n",
    "  if opt==0:\n",
    "    NInd = el+1 # pick one image from N images\n",
    "    Ns   = 4 # Number of channels common to all images\n",
    "    Nz   = 0 # Number of image specific channels\n",
    "    Nout = 1 # Number of outputs, 1 or N (2*Nout = output channels) \n",
    "    Nin = Nz+Ns\n",
    "  elif opt==1: # Fix noise stack in with image specific output channels\n",
    "    Ns   = 4 # Number of channels common to all images\n",
    "    Nz   = 0 # Number of image specific channels\n",
    "    Nout = N # Number of outputs, 1 or N (2*Nout = output channels)\n",
    "    Nin = Nz+Ns\n",
    "  elif opt==2: # [Ns; Nz] in with a single output channel\n",
    "    Ns   = 3 # Number of channels common to all images #<--3\n",
    "    Nz   = 1 # Number of image specific channels #<--1\n",
    "    Nout = 1 # Number of outputs, 1 or N (2*Nout = output channels)\n",
    "    Nin = Nz+Ns\n",
    "  elif opt==3: # [Ns; Nz] in with image specific output channels\n",
    "    Ns   = 3 # Number of channels common to all images\n",
    "    Nz   = 1 # Number of image specific channels\n",
    "    Nout = N # Number of outputs, 1 or N (2*Nout = output channels)\n",
    "    Nin = Nz+Ns\n",
    "  elif opt==4: # [Ns + Nz] in with a single output channel\n",
    "    Ns   = 1 # Number of channels common to all images \n",
    "    Nz   = Ns # Number of image specific channels\n",
    "    Nout = 1 # Number of outputs, 1 or N (2*Nout = output channels)\n",
    "    Nin = Nz\n",
    "  elif opt==5: # H[Nz] in with image-specific first layer and single output channel \n",
    "    Ns   = 0 # Number of channels common to all images\n",
    "    Nz   = 2 # Number of image specific channels\n",
    "    Nout = 1 # Number of outputs, 1 or N (2*Nout = output channels)\n",
    "    Nin = Nz*N\n",
    "  elif opt==6: # H[Nz] in with image-specific first layer and image specific output channels\n",
    "    Ns   = 0 # Number of channels common to all images\n",
    "    Nz   = 2 # Number of image specific channels\n",
    "    Nout = N # Number of outputs, 1 or N (2*Nout = output channels)\n",
    "    Nin = Nz*N\n",
    "\n",
    "\n",
    "  if NInd==0: # process all images\n",
    "    x  = xN\n",
    "    y  = yN\n",
    "    yn = ynN\n",
    "    yu = yuN\n",
    "    msk= mskN\n",
    "  elif NInd > 0: # Process only one image\n",
    "    N  = 1\n",
    "    x  = xN[(NInd-1)*2:(NInd-1)*2+2]\n",
    "    y  = yN[(NInd-1)*2:(NInd-1)*2+2]\n",
    "    yn = ynN[(NInd-1)*2:(NInd-1)*2+2]\n",
    "    yu = yuN[(NInd-1)*2:(NInd-1)*2+2]\n",
    "    msk= mskN[(NInd-1):NInd]\n",
    "\n",
    "\n",
    "\n",
    "  ########################################################## Network setup\n",
    "  pad = 'reflection' # 'zero'\n",
    "  mse = torch.nn.MSELoss().type(dtype)\n",
    "  mae = torch.nn.L1Loss().type(dtype)\n",
    "\n",
    "  INPUT = 'noise' # 'noise', 'meshgrid', or 'hybrid'\n",
    "\n",
    "  # ra: note, setting num_channels_skip[0] = 0 may improve performance\n",
    "  if opt==5 or opt==6: \n",
    "    net = skip(Nin, 2*Nout, # skip|skip_depth6|skip_depth4|skip_depth2|UNET|ResNet\n",
    "    num_channels_down = [2]+[LSz]*(NLy-1), #[2, 128, 128, 128, 128, 128]\n",
    "    # num_channels_down[0] = 2,\n",
    "    num_channels_up =   [LSz]*NLy, #[128, 128, 128, 128, 128, 128]\n",
    "    num_channels_skip = [LSz]*NLy, #[128, 128, 128, 128, 128, 128] \n",
    "    filter_size_up   = [3]*NLy, #[3, 3, 3, 3, 3, 3], \n",
    "    filter_size_down = [3]*NLy, #[3, 3, 3, 3, 3, 3],\n",
    "    filter_skip_size = 1, # kernel size for the filters along the skip connections\n",
    "    upsample_mode='nearest', \n",
    "    output_act = 0, # 0 for none, 1 for sigmoid, 2 for tanh\n",
    "    need_bias=True, \n",
    "    pad=pad, \n",
    "    act_fun='LeakyReLU').type(dtype) # need_sigmoid forces the out to between 0 and 1\n",
    "  else:\n",
    "    # RA: The number of layers = 2^layers < image size\n",
    "    # Five layers for sl64 and six for other applications\n",
    "    net = skip(Nin, 2*Nout, # skip|skip_depth6|skip_depth4|skip_depth2|UNET|ResNet\n",
    "    num_channels_down = [LSz]*NLy, #[128, 128, 128, 128, 128]\n",
    "    num_channels_up =   [LSz]*NLy, #[128, 128, 128, 128, 128]\n",
    "    num_channels_skip = [LSz]*NLy, #[128, 128, 128, 128, 128]  \n",
    "    filter_size_up   = [3]*NLy, #[3, 3, 3, 3, 3], \n",
    "    filter_size_down = [3]*NLy, #[3, 3, 3, 3, 3],\n",
    "    filter_skip_size = 1, # kernel size for the filters along the skip connections\n",
    "    upsample_mode='nearest', \n",
    "    output_act = 0, # 0 for none, 1 for sigmoid, 2 for tanh\n",
    "    need_bias=True, \n",
    "    pad=pad, \n",
    "    act_fun='LeakyReLU').type(dtype) # need_sigmoid forces the out to between 0 and 1\n",
    "\n",
    "\n",
    "  s  = sum(np.prod(list(p.size())) for p in net.parameters())\n",
    "  print ('Number of params: %d' % s)\n",
    "\n",
    "\n",
    "  ########################################################## Setup network inputs\n",
    "  # generate network input\n",
    "  x_tor   = np_to_torch(x).type(dtype)\n",
    "  yu_tor  = np_to_torch(yu).type(dtype)\n",
    "  yn_tor  = np_to_torch(yn).type(dtype)\n",
    "  msk_tor = np_to_torch(msk).type(dtype)\n",
    "  # x_avg_tor = np_to_torch(x_avg).type(dtype)\n",
    "\n",
    "  net = net.type(dtype)\n",
    "  if opt==5 or opt==6:\n",
    "    # net = net.type(dtype)\n",
    "    z0 = torch.zeros(1,Nz,n[0],n[1]).type(dtype) # all zeros\n",
    "    z = get_noise(Nz, INPUT, x.shape[1:], var=1/10).type(dtype) - 1/20\n",
    "    z_saved = torch.clone(z)   # ra: use clone so that z and z_saved don't point to the same location\n",
    "    z0_saved = torch.clone(z0)\n",
    "    # zs = get_noise(Nin-Nz, INPUT, x.shape[1:], var=1./10).type(dtype) - 1/20\n",
    "    # zs_saved = torch.clone(zs)  # ra: use clone so that zs and zs_saved don't point to the same location\n",
    "\n",
    "  elif opt==4:\n",
    "    z = torch.zeros(1,Nz*N,n[0],n[1]).type(dtype) # all zeros\n",
    "    zs = get_noise(Ns, INPUT, x.shape[1:], var=1./10).type(dtype) - 1/20\n",
    "    z_saved  = torch.clone(z)\n",
    "    zs_saved = torch.clone(zs)\n",
    "    z0 = torch.zeros(1,Nz*N,n[0],n[1]).type(dtype) # all zeros\n",
    "    z0_saved = torch.clone(z0)\n",
    "\n",
    "  else: \n",
    "    zs = get_noise(Nin-Nz, INPUT, x.shape[1:], var=1./10).type(dtype) - 1/20\n",
    "    zs_saved = torch.clone(zs)  # ra: use clone so that zs and zs_saved don't point to the same location\n",
    "    \n",
    "    z0 = get_noise(Nz, INPUT, x.shape[1:], var=1./10).type(dtype) - 1/20\n",
    "    z = torch.zeros(1,Nz*N,n[0],n[1]).type(dtype) # all zeros\n",
    "    for i in range(N):\n",
    "      z[:,i*Nz:(i+1)*Nz,:,:] = z_sc*(0.8*z0 + 0.2*(get_noise(Nz, INPUT, x.shape[1:], var=1./10).type(dtype) - 1/20))\n",
    "    z0 = torch.tile(z0,[1,N,1,1])\n",
    "    z_saved  = torch.clone(z)   # ra: use clone so that z and z_saved don't point to the same location\n",
    "    z0_saved = torch.clone(z0)\n",
    "\n",
    "  # print(z.shape)\n",
    "\n",
    "\n",
    "  ########################################################## Main iterations\n",
    "  # from torch.nn.modules.loss import L1Loss\n",
    "  ii = 0\n",
    "  def closure():\n",
    "      global ii\n",
    "      # global running_loss\n",
    "      # global z_lamb\n",
    "      z_lamb = z_lamb0 #*(1 + 99 * ii/num_iter)\n",
    "      losses = torch.empty([N,1]).type(dtype)\n",
    "      xHat_tor = torch.empty(N,1,2*Nout, n[0], n[1]).type(dtype)\n",
    "      reg_sig = reg_sig0*(1 - 0.9 * ii/num_iter)\n",
    "      for i in range(N):\n",
    "        if opt==5 or opt==6:\n",
    "          xHat_tor[i,:,:,:,:] = net(torch.cat((torch.randn(1,Nz*i,n[0],n[1]).type(dtype) * reg_sig/(N**0.5), z + (torch.randn(1,Nz,n[0],n[1]).type(dtype) * reg_sig), torch.randn(1,N*Nz-Nz*(i+1),n[0],n[1]).type(dtype)* reg_sig/(N**0.5)), 1))\n",
    "        elif opt==4:\n",
    "          xHat_tor[i,:,:,:,:] = net(zs + z[:,i*Nz:(i+1)*Nz,:,:] + (torch.randn(1,Nz,n[0],n[1]).type(dtype) * reg_sig))\n",
    "        else: \n",
    "          xHat_tor[i,:,:,:,:] = net(torch.cat((zs + (torch.randn(1,Nin-Nz,n[0],n[1]).type(dtype) * reg_sig), z[:,i*Nz:(i+1)*Nz,:,:] + (torch.randn(1,Nz,n[0],n[1]).type(dtype) * reg_sig)), 1)) #<-- change *1 to *4 AND <--zs to zs_saved\n",
    "        if Nout==N:\n",
    "          xHatF_tor = fft2c_ra(xHat_tor[i,:,i*2:(i+1)*2,:,:], 'ortho')\n",
    "        elif Nout<N:\n",
    "          xHatF_tor = fft2c_ra(xHat_tor[i,:,:,:,:], 'ortho')\n",
    "\n",
    "        losses[i] = mse(xHatF_tor*msk_tor[:,i:i+1,:,:],  yu_tor[:,i*2:(i+1)*2,:,:])\n",
    "        \n",
    "      if opt==2 or opt==3 or opt==4:\n",
    "        z_loss = z_lamb * torch.mean(torch.sqrt(torch.mean(torch.abs(z)**2, axis=1)+1e-6))\n",
    "        # z_loss = z_lamb*mae(z, z0_saved) # z_loss = mse(z,z0_saved) #<--default\n",
    "        # z_loss = z_lamb*mae(z, torch.tile(z[:,0:Nz,:,:],[1,N,1,1]))\n",
    "        # z_loss = z_lamb*mae(z, torch.tile(z[:,np.remainder(ii,N)*Nz:(np.remainder(ii,N)+1)*Nz,:,:],[1,N,1,1]))\n",
    "      else:\n",
    "        z_loss = 0\n",
    "\n",
    "      total_loss = sum(losses) + z_loss\n",
    "\n",
    "      total_loss.backward()\n",
    "      running_loss[0,ii] = total_loss.item()\n",
    "\n",
    "      # print ('Iteration %05d    Loss %f' % (i, total_loss.item()), '\\r', end='')\n",
    "      if  PLOT and (ii ==0 or (ii+1) % show_every == 0):\n",
    "        nmse = np.zeros([N,1])\n",
    "        ssm = np.zeros([N,1])\n",
    "        xHat = np.zeros((2*N,n[0],n[1]))\n",
    "        xHatAbs = np.zeros((N,n[0],n[1]))\n",
    "        xAbs = np.zeros((N,n[0],n[1]))\n",
    "        errMap = np.zeros((2*N,n[0],n[1]))\n",
    "        for i in range(N):\n",
    "          if Nout==N:\n",
    "            xHat[i*2:(i+1)*2,:,:] = torch_to_np(xHat_tor[i, :, i*2:(i+1)*2,:,:])\n",
    "            xHatAbs[i:i+1,:,:] = np.sqrt((xHat[i*2,:,:])**2 + (xHat[i*2+1,:,:])**2)\n",
    "            xAbs[i:i+1,:,:] = np.sqrt((x[i*2,:,:])**2 + (x[i*2+1,:,:])**2)\n",
    "          elif Nout<N:\n",
    "            xHat[i*2:(i+1)*2,:,:] = torch_to_np(xHat_tor[i,:,:,:,:])\n",
    "            xHatAbs[i:i+1,:,:] = np.sqrt((xHat[i*2,:,:])**2 + (xHat[i*2+1,:,:])**2)\n",
    "            xAbs[i:i+1,:,:] = np.sqrt((x[i*2,:,:])**2 + (x[i*2+1,:,:])**2)\n",
    "          \n",
    "          nmse[i] =  np.mean((x[i*2:(i+1)*2,:,:]-xHat[i*2:(i+1)*2,:,:])**2) / np.mean((x[i*2:(i+1)*2,:,:])**2)\n",
    "          ssm[i] = ssim(xHatAbs[i,:,:], xAbs[i,:,:], data_range = xHatAbs[i,:,:].max() - xHatAbs[i,:,:].min()) # xHatL1Abs[i:i+1,:,:].max() - xHatL1Abs[i:i+1,:,:].min() \n",
    "          errMap[i*2:(i+1)*2,:,:] = x[i*2:(i+1)*2,:,:]-xHat[i*2:(i+1)*2,:,:]\n",
    "        \n",
    "        print('Individual losses x 1e4: ',', '.join('%1.3f' % (losses[j]*1e4) for j in range(len(losses))))\n",
    "        print('Iteration: %1.3d,' %(ii+1), 'Loss x 1e4: %1.2f,' %(running_loss[0,ii]*1e4), 'Mean nmse: %1.2f,' %(10*np.log10(np.mean(nmse))), 'nmse: ',', '.join('%1.2f' % (10*np.log10(nmse[j])) for j in range(len(nmse)))) \n",
    "        fig = plt.figure(figsize=(16,5),facecolor='white', edgecolor=None)\n",
    "        plt.imshow(np.reshape(np.transpose(xHat,[1,0,2]), [n[0],n[1]*2*N]), vmin=-1, vmax=1, cmap=plt.cm.Greys_r) # use a specific color map\n",
    "        plt.show()\n",
    "\n",
    "        fig = plt.figure(figsize=(16,5),facecolor='white', edgecolor=None)\n",
    "        plt.imshow(np.reshape(np.transpose(errMap,[1,0,2]), [n[0],n[1]*2*N]), vmin=-0.1, vmax=0.1, cmap=plt.cm.Greys_r) # use a specific color map\n",
    "        plt.show()\n",
    "\n",
    "        fig = plt.figure(figsize=(16,5),facecolor='white', edgecolor=None)\n",
    "        if Nz == 0:\n",
    "          plt.imshow(np.reshape(np.transpose(np.concatenate((xAbs[0:1,:,:]**gm, xHatAbs[0:1,:,:]**gm), axis=2),[1,0,2]), [n[0],n[1]*2]), vmin=0, vmax=0.7, cmap=plt.cm.Greys_r) # use a specific color map\n",
    "          plt.show() \n",
    "        else:\n",
    "          zNp = torch_to_np(z)\n",
    "          plt.imshow(np.reshape(np.transpose(np.concatenate((xAbs[0:1,:,:]**gm, xHatAbs[0:1,:,:]**gm, 50*zNp[0:1,:,:]+0.7/2), axis=2),[1,0,2]), [n[0],n[1]*3]), vmin=0, vmax=0.7, cmap=plt.cm.Greys_r) # use a specific color map\n",
    "          plt.show()\n",
    "      ii += 1\n",
    "      return total_loss\n",
    "\n",
    "\n",
    "  # num_iter = 15000 #12000\n",
    "  running_loss = torch.empty([1,num_iter]).type(dtype)\n",
    "  # reg_sig0 = 0.02 * (1 + (Np*1e3)**0.5) \n",
    "\n",
    "\n",
    "  print('=====optimizing over network and input=====')\n",
    "  OPT_OVER = 'net,input'\n",
    "  # z_lamb0 = 8*2e0 #<-- default: 5e1 for mse, 2e0 for mae, and 2e0 for group sparsity\n",
    "  # WtD = 0*1e-6 # weight decay\n",
    "  if Nz != 0 and Ns != 0:\n",
    "    p = get_params(OPT_OVER, net, ([z,zs])) #<-- ra: remove zs\n",
    "  if Nz != 0 and Ns == 0:\n",
    "    p = get_params(OPT_OVER, net, ([z])) #<-- ra: remove zs\n",
    "  if Ns != 0 and Nz == 0:\n",
    "    p = get_params(OPT_OVER, net, ([zs])) #<-- ra: remove zs\n",
    "  optimize('adam', p, closure, LR, num_iter, WtD)\n",
    "\n",
    "\n",
    "  fig = plt.figure(figsize=(16,5),facecolor='white', edgecolor=None)\n",
    "  plt.plot(np.log10(torch_to_np(running_loss)))\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "  ########################################################## Display and saving\n",
    "  xHat_tor = torch.empty(N,1,2*Nout, n[0], n[1]).type(dtype)\n",
    "  for i in range(N):\n",
    "    if opt==5 or opt==6:\n",
    "      xHat_tor[i,:,:,:,:] = net(torch.cat((torch.zeros(1,Nz*i,n[0],n[1]).type(dtype), z , torch.zeros(1,N*Nz-Nz*(i+1),n[0],n[1]).type(dtype)), 1))\n",
    "    elif opt==4:\n",
    "      xHat_tor[i,:,:,:,:] = net(1*zs + 1*z[:,i*Nz:(i+1)*Nz,:,:])\n",
    "    else: \n",
    "      xHat_tor[i,:,:,:,:] = net(torch.cat((1*zs, 1*z[:,i*Nz:(i+1)*Nz,:,:]), 1))\n",
    "\n",
    "  nmse = np.zeros([N,1])\n",
    "  ssm = np.zeros([N,1])\n",
    "  xHat = np.zeros((2*N,n[0],n[1]))\n",
    "  xHatAbs = np.zeros((N,n[0],n[1]))\n",
    "  xHatF = np.zeros((2*N,n[0],n[1]))\n",
    "  xHatFAbs = np.zeros((N,n[0],n[1]))\n",
    "  xAbs = np.zeros((N,n[0],n[1]))\n",
    "  errMap = np.zeros((N,n[0],n[1]))\n",
    "  errFMap = np.zeros((N,n[0],n[1]))\n",
    "\n",
    "  for i in range(N):\n",
    "    if Nout==N:\n",
    "      xHat[i*2:(i+1)*2,:,:] = torch_to_np(xHat_tor[i, :, i*2:(i+1)*2,:,:])\n",
    "      xHatAbs[i:i+1,:,:] = np.sqrt((xHat[i*2,:,:])**2 + (xHat[i*2+1,:,:])**2)\n",
    "      xHatF[i*2:(i+1)*2,:,:] = torch_to_np(fft2c_ra(xHat_tor[i, :, i*2:(i+1)*2,:,:],'ortho'))\n",
    "      xHatFAbs[i:i+1,:,:] = np.sqrt((xHatF[i*2,:,:])**2 + (xHatF[i*2+1,:,:])**2)\n",
    "      xAbs[i:i+1,:,:] = np.sqrt((x[i*2,:,:])**2 + (x[i*2+1,:,:])**2)\n",
    "    elif Nout<N:\n",
    "      xHat[i*2:(i+1)*2,:,:] = torch_to_np(xHat_tor[i,:,:,:,:])\n",
    "      xHatAbs[i:i+1,:,:] = np.sqrt((xHat[i*2,:,:])**2 + (xHat[i*2+1,:,:])**2)\n",
    "      xHatF[i*2:(i+1)*2,:,:] = torch_to_np(fft2c_ra(xHat_tor[i,:,:,:,:],'ortho'))\n",
    "      xHatFAbs[i:i+1,:,:] = np.sqrt((xHatF[i*2,:,:])**2 + (xHatF[i*2+1,:,:])**2)\n",
    "      xAbs[i:i+1,:,:] = np.sqrt((x[i*2,:,:])**2 + (x[i*2+1,:,:])**2)\n",
    "    \n",
    "    nmse[i] =  np.mean((x[i*2:(i+1)*2,:,:]-xHat[i*2:(i+1)*2,:,:])**2) / np.mean((x[i*2:(i+1)*2,:,:])**2)\n",
    "    ssm[i] = ssim(xHatAbs[i,:,:], xAbs[i,:,:], data_range = xHatAbs[i,:,:].max() - xHatAbs[i,:,:].min()) # xHatL1Abs[i:i+1,:,:].max() - xHatL1Abs[i:i+1,:,:].min() \n",
    "    errMap[i,:,:] = np.sqrt(np.sum(np.abs(x[i*2:(i+1)*2,:,:]-xHat[i*2:(i+1)*2,:,:])**2, axis=0))\n",
    "    errFMap[i,:,:] = np.sqrt(np.sum(np.abs(y[i*2:(i+1)*2,:,:]-xHatF[i*2:(i+1)*2,:,:])**2, axis=0))\n",
    "    \n",
    "  print('Mean nmse: %1.2f,' %(10*np.log10(np.mean(nmse))), 'nmse: ',', '.join('%1.2f' % (10*np.log10(nmse[j])) for j in range(len(nmse)))) \n",
    "  print('Mean ssim: %1.3f,' %(np.mean(ssm)), 'ssim: ',', '.join('%1.3f' % (ssm[j]) for j in range(len(ssm)))) \n",
    "\n",
    "  fig = plt.figure(figsize=(16,5),facecolor='white', edgecolor=None)\n",
    "  plt.imshow(np.reshape(np.transpose(xHatFAbs**0.25,[1,0,2]), [n[0],n[1]*N]), cmap=plt.cm.Greys_r) # use a specific color map\n",
    "  plt.show()\n",
    "  fig = plt.figure(figsize=(16,5),facecolor='white', edgecolor=None)\n",
    "  plt.imshow(np.reshape(np.transpose(errFMap**gm,[1,0,2]), [n[0],n[1]*N]), cmap=plt.cm.Greys_r) # use a specific color map\n",
    "  plt.show() \n",
    "\n",
    "  fig = plt.figure(figsize=(16,5),facecolor='white', edgecolor=None)\n",
    "  plt.imshow(np.reshape(np.transpose(xHatAbs**gm,[1,0,2]), [n[0],n[1]*N]), vmin=0, vmax=0.7, cmap=plt.cm.Greys_r) # use a specific color map\n",
    "  plt.show()\n",
    "  fig = plt.figure(figsize=(16,5),facecolor='white', edgecolor=None)\n",
    "  plt.imshow(np.reshape(np.transpose(errMap**gm,[1,0,2]), [n[0],n[1]*N]), vmin=0, vmax=0.2, cmap=plt.cm.Greys_r) # use a specific color map\n",
    "  plt.show()\n",
    "\n",
    "  if sv==1: np.save(data_path + 'xRefDIP_'+'opt_%d_N_%d_Ind_%d' % (opt, N, NInd) + '.npy', x)\n",
    "  if sv==1: np.save(data_path + 'xHatDIP_'+'opt_%d_N_%d_Ind_%d' % (opt, N, NInd) + '.npy', xHat)\n",
    "  if sv==1: np.save(data_path + 'nmseDIP_'+'opt_%d_N_%d_Ind_%d' % (opt, N, NInd) + '.npy', nmse)\n",
    "  if sv==1: np.save(data_path + 'ssimDIP_'+'opt_%d_N_%d_Ind_%d' % (opt, N, NInd) + '.npy', ssm)\n",
    "  if Nz!=0:\n",
    "    if sv==1: np.save(data_path + 'zDIP_'+'opt_%d_N_%d_Ind_%d' % (opt, N, NInd) + '.npy', torch_to_np(z))\n",
    "  if Ns!=0: \n",
    "    if sv==1: np.save(data_path + 'zsDIP_'+'opt_%d_N_%d_Ind_%d' % (opt, N, NInd) + '.npy', torch_to_np(zs))\n",
    "\n",
    "  if sv==1: torch.save(net, data_path + 'model_'+'opt_%d_N_%d_Ind_%d' % (opt, N, NInd))\n",
    "  # model = torch.load(data_path + 'model_'+'opt_%d_N_%d_Ind_%d' % (opt, N, NInd))\n",
    "  # model.eval()\n",
    "\n",
    "\n",
    "  ########################################################## Display/read the weights\n",
    "  # # Print model's state_dict\n",
    "  # print(\"Model's state_dict:\")\n",
    "  # for param_tensor in net.state_dict():\n",
    "  #     print(param_tensor, \"\\t\", net.state_dict()[param_tensor].size())\n",
    "\n",
    "  # # Print optimizer's state_dict\n",
    "  # print(\"Optimizer's state_dict:\")\n",
    "  # for var_name in optimizer.state_dict():\n",
    "  #     print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n",
    "\n",
    "  # print(net) # this gives the structure of the network\n",
    "  L = 4 # read thie weights of this layer\n",
    "  i=0\n",
    "  for param in net.parameters():\n",
    "    wt = param.data\n",
    "    if i == L:\n",
    "      break\n",
    "    i=i+1\n",
    "\n",
    "  print(wt.shape)\n",
    "  print(wt.view(-1)) # this view(-1) reshpaes into a list\n",
    "  # print(zs.shape)\n",
    "\n",
    "\n",
    "  ########################################################## Display code vectors\n",
    "  if Ns !=0: # plotting 1 but there are Ns=3 static channels\n",
    "    fig = plt.figure(figsize=(16,5),facecolor='white', edgecolor=None)\n",
    "    plt.imshow(np.concatenate((torch_to_np(zs_saved[:,0,:,:]), torch_to_np(zs[:,0,:,:]), torch_to_np(zs[:,0,:,:]-zs_saved[:,0,:,:])), axis=1), cmap=plt.cm.Greys_r) # use a specific color map\n",
    "    plt.show()\n",
    "    print(np.max(torch_to_np(zs[:,0,:,:]-zs_saved[:,0,:,:])))\n",
    "\n",
    "  if Nz !=0:\n",
    "    for i in range(N*Nz):\n",
    "      fig = plt.figure(figsize=(16,5),facecolor='white', edgecolor=None)\n",
    "      plt.imshow(np.concatenate((torch_to_np(z_saved[:,i,:,:]), torch_to_np(z[:,i,:,:]), torch_to_np(z[:,i,:,:]-z_saved[:,i,:,:])) , axis=1), cmap=plt.cm.Greys_r) # use a specific color map\n",
    "      plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "  # sv=1\n",
    "  # if sv:\n",
    "  if sv==1: np.save(data_path_r + 'xRefDISCUS_'+'N_%d_LR_%f_zlamb_%f_zSc_%f' % (N, LR, z_lamb0, z_sc) + '_R_%f'%R + '.npy', x)\n",
    "  if sv==1: np.save(data_path_r + 'xHatDISCUS_'+'N_%d_LR_%f_zlamb_%f_zSc_%f' % (N, LR, z_lamb0, z_sc)+ '_R_%f'%R + '.npy', xHat)\n",
    "  if Nz!=0:\n",
    "    if sv==1: np.save(data_path_r + 'zDISCUS_'+'N_%d_LR_%f_zlamb_%f_zSc_%f' % (N, LR, z_lamb0, z_sc)+ '_R_%f'%R + '.npy', torch_to_np(z))\n",
    "  if Ns!=0: \n",
    "    if sv==1: np.save(data_path_r + 'zsDISCUS_'+'N_%d_LR_%f_zlamb_%f_zSc_%f' % (N, LR, z_lamb0, z_sc) + '_R_%f'%R + '.npy', torch_to_np(zs))\n",
    "  if sv==1: torch.save(net, data_path_r + 'model_DISCUS_'+'N_%d_LR_%f_zlamb_%f_zSc_%f' % (N, LR, z_lamb0, z_sc)+ '_R_%f'%R )\n",
    "    \n",
    "  # get the end time\n",
    "  et = time.time()\n",
    "\n",
    "  # get the execution time\n",
    "  elapsed_time = et - st\n",
    "  print('Execution time:', elapsed_time/60, 'minutes')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "",
   "version": ""
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "DISCUS-Sultan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "552489c190a5836f1d6d306c55383d234ab7e9c654141c6542f91293c67cd02a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
